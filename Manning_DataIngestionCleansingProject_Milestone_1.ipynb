{"cells":[{"cell_type":"markdown","source":["# Data Ingestion and Cleansing\n## Milestone #1: Set Up Your Databricks Community Environment\n\nThis notebook contains the code needed to perform steps 3 and 4.\n\nRun the different cells to generate the desired outputs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68abb27f-4b47-4c26-aa3c-030c0f88f1a0"}}},{"cell_type":"code","source":["# Making sure that everything is working as expected\nprint('Hello World!')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2467b998-6370-4588-9015-9b1c11d8865d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Hello World!\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Hello World!\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# There are multiple ways to print the configuration of our Spark session\n# Two methods that print all the parameters are methods #2 and #3\n# Method #1 introduces the SparkConf class, which is very useful to update parameters\n\n# Method #1: Using SparkConf\nfrom pyspark import SparkConf\ndisplay(SparkConf().getAll())\n\n# Method #2: Using sql('SET -v')\ndisplay(spark.sql(\"SET -v\"))\n\n# Method #3: Using SparkContext.getConf().getAll()\ndisplay(spark.sparkContext.getConf().getAll())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Step #3","showTitle":true,"inputWidgets":{},"nuid":"09ed9c96-0405-4f6a-ba2d-c0036cf693b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["spark.executor.memory","8278m"],["spark.ui.port","46359"],["spark.executor.extraJavaOptions","-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1"],["spark.executor.extraClassPath","/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.joshelser--dropwizard-metrics-hadoop-metrics2-reporter--com.github.joshelser__dropwizard-metrics-hadoop-metrics2-reporter__0.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-common--org.apache.hive__hive-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-exec-core--org.apache.hive__hive-exec-core__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-vector-code-gen--org.apache.hive__hive-vector-code-gen__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.mesos--mesos-shaded-protobuf--org.apache.mesos__mesos-shaded-protobuf__1.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-core--org.apache.orc__orc-core__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-mapreduce--org.apache.orc__orc-mapreduce__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.xbean--xbean-asm7-shaded--org.apache.xbean__xbean-asm7-shaded__4.15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util-ajax--org.eclipse.jetty__jetty-util-ajax__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--jakarta.inject--org.glassfish.hk2.external__jakarta.inject__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.inject--jersey-hk2--org.glassfish.jersey.inject__jersey-hk2__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.hibernate.validator--hibernate-validator--org.hibernate.validator__hibernate-validator__6.1.0.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.javassist--javassist--org.javassist__javassist__3.25.0-GA.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.3.2.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-ast_2.12--org.json4s__json4s-ast_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-core_2.12--org.json4s__json4s-core_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-jackson_2.12--org.json4s__json4s-jackson_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-scalap_2.12--org.json4s__json4s-scalap_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.lz4--lz4-java--org.lz4__lz4-java__1.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.2.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--shims--org.roaringbitmap__shims__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__6.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-compiler_2.12--org.scala-lang__scala-compiler__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-library_2.12--org.scala-lang__scala-library__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-reflect_2.12--org.scala-lang__scala-reflect__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-collection-compat_2.12--org.scala-lang.modules__scala-collection-compat_2.12__2.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-parser-combinators_2.12--org.scala-lang.modules__scala-parser-combinators_2.12__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-xml_2.12--org.scala-lang.modules__scala-xml_2.12__1.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalacheck--scalacheck_2.12--org.scalacheck__scalacheck_2.12__1.14.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalactic--scalactic_2.12--org.scalactic__scalactic_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze-macros_2.12--org.scalanlp__breeze-macros_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze_2.12--org.scalanlp__breeze_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalatest--scalatest_2.12--org.scalatest__scalatest_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.threeten--threeten-extra--org.threeten__threeten-extra__1.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.tukaani--xz--org.tukaani__xz__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--algebra_2.12--org.typelevel__algebra_2.12__2.0.0-M2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--cats-kernel_2.12--org.typelevel__cats-kernel_2.12__2.0.0-M4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--machinist_2.12--org.typelevel__machinist_2.12__0.6.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--macro-compat_2.12--org.typelevel__macro-compat_2.12__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-macros_2.12--org.typelevel__spire-macros_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-platform_2.12--org.typelevel__spire-platform_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-util_2.12--org.typelevel__spire-util_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire_2.12--org.typelevel__spire_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.wildfly.openssl--wildfly-openssl--org.wildfly.openssl__wildfly-openssl__1.0.7.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.yaml--snakeyaml--org.yaml__snakeyaml__1.24.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--pl.edu.icm--JLargeArrays--pl.edu.icm__JLargeArrays__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--stax--stax-api--stax__stax-api__1.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.yetus--audience-annotations--org.apache.yetus__audience-annotations__0.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.14.jar:/databricks/jars/----workspace_spark_3_1--mllib--libmllib_resources.jar:/databricks/jars/----workspace_spark_3_1--mllib--mllib_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--mllib--org.jpmml__pmml-model__1.4.8_shaded-for-mllib.jar:/databricks/jars/----workspace_spark_3_1--mllib-local--mllib-local-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--py4j--py4j-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--repl--repl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--catalyst-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libcatalyst_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libspark-sql-parser-compiled.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--core--proto_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--spark-sql-databricks-command-parser_java_out.srcjar:/databricks/jars/----workspace_spark_3_1--sql--hive--hive_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--libhive_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--org.apache.commons__commons-pool2__2.6.2_shaded-for-hive.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-protocol-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--streaming--libstreaming_resources.jar:/databricks/jars/----workspace_spark_3_1--streaming--streaming-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--fatJar-assembly-0.18.1-SNAPSHOT.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--gcs-connector-hadoop2-2.0.0-shaded.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core-http-netty__1.6.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core__1.8.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-identity__1.1.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-security-keyvault-keys__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.dataformat__jackson-dataformat-xml__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.datatype__jackson-datatype-jsr310__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.module__jackson-module-jaxb-annotations__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.woodstox__woodstox-core__6.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml__aalto-xml__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.github.stephenc.jcip__jcip-annotations__1.0-1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.madgag.spongycastle__core__1.54.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j-persistence-extension__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j__1.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.sqlserver__mssql-jdbc__9.2.1.jre8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__content-type__2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__lang-tag__1.4.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__nimbus-jose-jwt__8.8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__oauth2-oidc-sdk__7.1.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-buffer__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-resolver__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-kqueue-osx-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor.netty__reactor-netty__0.9.11.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor__reactor-core__3.3.9.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.activation__jakarta.activation-api__1.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.xml.bind__jakarta.xml.bind-api__2.3.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.java.dev.jna__jna-platform__5.6.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__accessors-smart__1.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__json-smart__2.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.bouncycastle__bcprov-jdk15on__1.69_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.codehaus.woodstox__stax2-api__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.jetbrains__annotations__15.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-dom__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-jaxb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdbx__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-simple__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__database__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.nanohttpd__nanohttpd__2.3.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.ow2.asm__asm__5.0.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.simpleframework__simple-xml__2.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--stax__stax__1.2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--xpp3__xpp3__1.1.3.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--avro_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--libavro_resources_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--file-notification-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--libfile-notification-common_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider--libkafka-0-10-token-provider_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider-unshaded_2.12_deploy_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--libkafka-0-10-resources_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--org.apache.kafka__kafka-clients__2.6.0_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--redshift--redshift-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--libmetrics-ganglia.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--spark-ganglia-lgpl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--libsql-aws-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--sql-aws-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--libsql-azure-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--sql-azure-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-dw--sql-dw-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/acl--api--helpers--helpers-spark_3.1_2.12_deploy.jar:/databricks/jars/acl--auth--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/api--common--workspace-spark_3.1_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base_java-spark_3.1_2.12_deploy.jar:/databricks/jars/central--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.1_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--cloudstorage--presigned-url-spark_3.1_2.12_deploy.jar:/databricks/jars/common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.1_2.12_deploy.jar:/databricks/jars/common--crypto-providers--amazon-corretto-crypto-provider--libamazon-corretto-crypto-provider.jar:/databricks/jars/common--hadoop--hadoop-spark_3.1_2.12_deploy.jar:/databricks/jars/common--java-flight-recorder--java-flight-recorder-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jupyter-utils--jupyter_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.1_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--network--network-spark_3.1_2.12_deploy.jar:/databricks/jars/common--node-types--node-types-spark_3.1_2.12_deploy.jar:/databricks/jars/common--path--path-spark_3.1_2.12_deploy.jar:/databricks/jars/common--pricing--pricing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.1_2.12_deploy.jar:/databricks/jars/common--reflection--reflection-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage-driver-utils--storage_driver_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--util--locks-spark_3.1_2.12_deploy.jar:/databricks/jars/credentials-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/jsonutil--jsonutil-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--ratelimitedlogger--ratelimitedlogger-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--sourcecode--sourcecode-spark_3.1_2.12_deploy.jar:/databricks/jars/managed-catalog--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/maven-trees--amazon-corretto-crypto-provider--software.amazon.cryptools--AmazonCorrettoCryptoProvider-linux-x86_64--software.amazon.cryptools__AmazonCorrettoCryptoProvider-linux-x86_64__1.4.0.jar:/databricks/jars/s3--s3-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/secret-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--command-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.1_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__2.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.4.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__30.0-android_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.14.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.6.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.23.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_javax.annotation_javax.annotation-api_javax.annotation__javax.annotation-api__1.3.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.2.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/third_party--armeria--service_discovery-resources.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--annotations_shaded.jar:/databricks/jars/third_party--gcs-private--api-common_shaded.jar:/databricks/jars/third_party--gcs-private--auto-value-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--checker-compat-qual_shaded.jar:/databricks/jars/third_party--gcs-private--checker-qual_shaded.jar:/databricks/jars/third_party--gcs-private--commons-codec_shaded.jar:/databricks/jars/third_party--gcs-private--commons-lang3_shaded.jar:/databricks/jars/third_party--gcs-private--commons-logging_shaded.jar:/databricks/jars/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--gcs-private--error_prone_annotations_shaded.jar:/databricks/jars/third_party--gcs-private--failureaccess_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-system-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-connector_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-shaded-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcsio_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-storage_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--gcs-private--google-extensions_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-alts_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-api_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-auth_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-context_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-core_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-grpclb_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-stub_shaded.jar:/databricks/jars/third_party--gcs-private--gson_shaded.jar:/databricks/jars/third_party--gcs-private--guava_shaded.jar:/databricks/jars/third_party--gcs-private--httpclient_shaded.jar:/databricks/jars/third_party--gcs-private--httpcore_shaded.jar:/databricks/jars/third_party--gcs-private--j2objc-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--jackson-core_shaded.jar:/databricks/jars/third_party--gcs-private--javax.annotation-api_shaded.jar:/databricks/jars/third_party--gcs-private--jsr305_shaded.jar:/databricks/jars/third_party--gcs-private--listenablefuture_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-api_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--gcs-private--perfmark-api_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java-util_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java_shaded.jar:/databricks/jars/third_party--gcs-private--util-hadoop_shaded.jar:/databricks/jars/third_party--gcs-private--util_shaded.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.1_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-annotations_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-core_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-databind_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-api_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-impl_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-jackson_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--zeromq--jeromq_shaded.jar:/databricks/jars/third_party--zeromq--jnacl_shaded.jar:/databricks/jars/utils--process_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.1_2.12_deploy.jar"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_1","type":"\"string\"","metadata":"{}"},{"name":"_2","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th></tr></thead><tbody><tr><td>spark.executor.memory</td><td>8278m</td></tr><tr><td>spark.ui.port</td><td>46359</td></tr><tr><td>spark.executor.extraJavaOptions</td><td>-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1</td></tr><tr><td>spark.executor.extraClassPath</td><td>/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.joshelser--dropwizard-metrics-hadoop-metrics2-reporter--com.github.joshelser__dropwizard-metrics-hadoop-metrics2-reporter__0.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-common--org.apache.hive__hive-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-exec-core--org.apache.hive__hive-exec-core__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-vector-code-gen--org.apache.hive__hive-vector-code-gen__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.mesos--mesos-shaded-protobuf--org.apache.mesos__mesos-shaded-protobuf__1.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-core--org.apache.orc__orc-core__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-mapreduce--org.apache.orc__orc-mapreduce__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.xbean--xbean-asm7-shaded--org.apache.xbean__xbean-asm7-shaded__4.15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util-ajax--org.eclipse.jetty__jetty-util-ajax__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--jakarta.inject--org.glassfish.hk2.external__jakarta.inject__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.inject--jersey-hk2--org.glassfish.jersey.inject__jersey-hk2__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.hibernate.validator--hibernate-validator--org.hibernate.validator__hibernate-validator__6.1.0.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.javassist--javassist--org.javassist__javassist__3.25.0-GA.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.3.2.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-ast_2.12--org.json4s__json4s-ast_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-core_2.12--org.json4s__json4s-core_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-jackson_2.12--org.json4s__json4s-jackson_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-scalap_2.12--org.json4s__json4s-scalap_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.lz4--lz4-java--org.lz4__lz4-java__1.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.2.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--shims--org.roaringbitmap__shims__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__6.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-compiler_2.12--org.scala-lang__scala-compiler__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-library_2.12--org.scala-lang__scala-library__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-reflect_2.12--org.scala-lang__scala-reflect__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-collection-compat_2.12--org.scala-lang.modules__scala-collection-compat_2.12__2.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-parser-combinators_2.12--org.scala-lang.modules__scala-parser-combinators_2.12__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-xml_2.12--org.scala-lang.modules__scala-xml_2.12__1.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalacheck--scalacheck_2.12--org.scalacheck__scalacheck_2.12__1.14.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalactic--scalactic_2.12--org.scalactic__scalactic_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze-macros_2.12--org.scalanlp__breeze-macros_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze_2.12--org.scalanlp__breeze_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalatest--scalatest_2.12--org.scalatest__scalatest_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.threeten--threeten-extra--org.threeten__threeten-extra__1.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.tukaani--xz--org.tukaani__xz__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--algebra_2.12--org.typelevel__algebra_2.12__2.0.0-M2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--cats-kernel_2.12--org.typelevel__cats-kernel_2.12__2.0.0-M4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--machinist_2.12--org.typelevel__machinist_2.12__0.6.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--macro-compat_2.12--org.typelevel__macro-compat_2.12__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-macros_2.12--org.typelevel__spire-macros_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-platform_2.12--org.typelevel__spire-platform_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-util_2.12--org.typelevel__spire-util_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire_2.12--org.typelevel__spire_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.wildfly.openssl--wildfly-openssl--org.wildfly.openssl__wildfly-openssl__1.0.7.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.yaml--snakeyaml--org.yaml__snakeyaml__1.24.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--pl.edu.icm--JLargeArrays--pl.edu.icm__JLargeArrays__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--stax--stax-api--stax__stax-api__1.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.yetus--audience-annotations--org.apache.yetus__audience-annotations__0.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.14.jar:/databricks/jars/----workspace_spark_3_1--mllib--libmllib_resources.jar:/databricks/jars/----workspace_spark_3_1--mllib--mllib_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--mllib--org.jpmml__pmml-model__1.4.8_shaded-for-mllib.jar:/databricks/jars/----workspace_spark_3_1--mllib-local--mllib-local-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--py4j--py4j-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--repl--repl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--catalyst-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libcatalyst_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libspark-sql-parser-compiled.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--core--proto_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--spark-sql-databricks-command-parser_java_out.srcjar:/databricks/jars/----workspace_spark_3_1--sql--hive--hive_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--libhive_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--org.apache.commons__commons-pool2__2.6.2_shaded-for-hive.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-protocol-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--streaming--libstreaming_resources.jar:/databricks/jars/----workspace_spark_3_1--streaming--streaming-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--fatJar-assembly-0.18.1-SNAPSHOT.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--gcs-connector-hadoop2-2.0.0-shaded.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core-http-netty__1.6.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core__1.8.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-identity__1.1.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-security-keyvault-keys__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.dataformat__jackson-dataformat-xml__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.datatype__jackson-datatype-jsr310__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.module__jackson-module-jaxb-annotations__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.woodstox__woodstox-core__6.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml__aalto-xml__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.github.stephenc.jcip__jcip-annotations__1.0-1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.madgag.spongycastle__core__1.54.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j-persistence-extension__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j__1.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.sqlserver__mssql-jdbc__9.2.1.jre8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__content-type__2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__lang-tag__1.4.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__nimbus-jose-jwt__8.8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__oauth2-oidc-sdk__7.1.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-buffer__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-resolver__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-kqueue-osx-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor.netty__reactor-netty__0.9.11.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor__reactor-core__3.3.9.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.activation__jakarta.activation-api__1.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.xml.bind__jakarta.xml.bind-api__2.3.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.java.dev.jna__jna-platform__5.6.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__accessors-smart__1.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__json-smart__2.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.bouncycastle__bcprov-jdk15on__1.69_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.codehaus.woodstox__stax2-api__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.jetbrains__annotations__15.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-dom__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-jaxb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdbx__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-simple__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__database__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.nanohttpd__nanohttpd__2.3.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.ow2.asm__asm__5.0.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.simpleframework__simple-xml__2.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--stax__stax__1.2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--xpp3__xpp3__1.1.3.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--avro_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--libavro_resources_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--file-notification-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--libfile-notification-common_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider--libkafka-0-10-token-provider_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider-unshaded_2.12_deploy_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--libkafka-0-10-resources_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--org.apache.kafka__kafka-clients__2.6.0_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--redshift--redshift-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--libmetrics-ganglia.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--spark-ganglia-lgpl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--libsql-aws-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--sql-aws-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--libsql-azure-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--sql-azure-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-dw--sql-dw-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/acl--api--helpers--helpers-spark_3.1_2.12_deploy.jar:/databricks/jars/acl--auth--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/api--common--workspace-spark_3.1_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base_java-spark_3.1_2.12_deploy.jar:/databricks/jars/central--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.1_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--cloudstorage--presigned-url-spark_3.1_2.12_deploy.jar:/databricks/jars/common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.1_2.12_deploy.jar:/databricks/jars/common--crypto-providers--amazon-corretto-crypto-provider--libamazon-corretto-crypto-provider.jar:/databricks/jars/common--hadoop--hadoop-spark_3.1_2.12_deploy.jar:/databricks/jars/common--java-flight-recorder--java-flight-recorder-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jupyter-utils--jupyter_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.1_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--network--network-spark_3.1_2.12_deploy.jar:/databricks/jars/common--node-types--node-types-spark_3.1_2.12_deploy.jar:/databricks/jars/common--path--path-spark_3.1_2.12_deploy.jar:/databricks/jars/common--pricing--pricing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.1_2.12_deploy.jar:/databricks/jars/common--reflection--reflection-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage-driver-utils--storage_driver_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--util--locks-spark_3.1_2.12_deploy.jar:/databricks/jars/credentials-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/jsonutil--jsonutil-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--ratelimitedlogger--ratelimitedlogger-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--sourcecode--sourcecode-spark_3.1_2.12_deploy.jar:/databricks/jars/managed-catalog--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/maven-trees--amazon-corretto-crypto-provider--software.amazon.cryptools--AmazonCorrettoCryptoProvider-linux-x86_64--software.amazon.cryptools__AmazonCorrettoCryptoProvider-linux-x86_64__1.4.0.jar:/databricks/jars/s3--s3-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/secret-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--command-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.1_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__2.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.4.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__30.0-android_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.14.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.6.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.23.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_javax.annotation_javax.annotation-api_javax.annotation__javax.annotation-api__1.3.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.2.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/third_party--armeria--service_discovery-resources.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--annotations_shaded.jar:/databricks/jars/third_party--gcs-private--api-common_shaded.jar:/databricks/jars/third_party--gcs-private--auto-value-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--checker-compat-qual_shaded.jar:/databricks/jars/third_party--gcs-private--checker-qual_shaded.jar:/databricks/jars/third_party--gcs-private--commons-codec_shaded.jar:/databricks/jars/third_party--gcs-private--commons-lang3_shaded.jar:/databricks/jars/third_party--gcs-private--commons-logging_shaded.jar:/databricks/jars/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--gcs-private--error_prone_annotations_shaded.jar:/databricks/jars/third_party--gcs-private--failureaccess_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-system-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-connector_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-shaded-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcsio_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-storage_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--gcs-private--google-extensions_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-alts_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-api_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-auth_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-context_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-core_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-grpclb_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-stub_shaded.jar:/databricks/jars/third_party--gcs-private--gson_shaded.jar:/databricks/jars/third_party--gcs-private--guava_shaded.jar:/databricks/jars/third_party--gcs-private--httpclient_shaded.jar:/databricks/jars/third_party--gcs-private--httpcore_shaded.jar:/databricks/jars/third_party--gcs-private--j2objc-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--jackson-core_shaded.jar:/databricks/jars/third_party--gcs-private--javax.annotation-api_shaded.jar:/databricks/jars/third_party--gcs-private--jsr305_shaded.jar:/databricks/jars/third_party--gcs-private--listenablefuture_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-api_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--gcs-private--perfmark-api_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java-util_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java_shaded.jar:/databricks/jars/third_party--gcs-private--util-hadoop_shaded.jar:/databricks/jars/third_party--gcs-private--util_shaded.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.1_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-annotations_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-core_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-databind_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-api_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-impl_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-jackson_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--zeromq--jeromq_shaded.jar:/databricks/jars/third_party--zeromq--jnacl_shaded.jar:/databricks/jars/utils--process_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.1_2.12_deploy.jar</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["spark.databricks.adaptive.autoBroadcastJoinThreshold","31457280b","Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join when adaptive execution is enabled. By setting this value to -1 broadcasting can be disabled.",""],["spark.databricks.adaptive.autoOptimizeShuffle.aggregateRatio","0.1","The aggregate output/input row ratio employed to estimate aggregate output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.aggregateSpillFactor","0.1","The ratio of spilling by aggregate compared to sort of the same input size used in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.equalityFilterSelectivity","0.1","The filter selectivity of a single equality predicate employed to estimate filter output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.exchangeSpillFactor","0.1","The ratio of spilling by exchange compared to sort of the same input size used in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.filterSelectivity","0.5","The filter selectivity of a single non-equality predicate employed to estimate filter output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.joinFactor","1.0","The co-efficient used by join output size estimation in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.joinSpillFactor","1.0","The ratio of spilling by shuffled hash join compared to sort of the same input size used in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.maxPartitionNumber","20480","The maximum initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.minFilterRatio","0.01","The minimum output/input row ratio employed to estimate filter output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.minPartitionNumber","<undefined>","The minimum initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.preshufflePartitionSizeInBytes","128MB","The per partition size used to calculate the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.",""],["spark.databricks.adaptive.autoOptimizeShuffle.useFileSize","true","When true, file sizes will be used to estimate the sizes of input relations in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true; otherwise, the catalog stats will be used for the estimate instead.",""],["spark.databricks.adaptive.skewJoin.spillProofPartitionThresholdInBytes","1GB","When 'spark.databricks.adaptive.skewJoin.spillProof.enabled' is true, a partition is considered as skewed if its size in bytes is larger than this threshold regardless of other skew join conditions.",""],["spark.databricks.adaptive.skewJoin.spillProofTargetPartitionSizeInBytes","512MB","The target partition size in bytes for splitting skew partitions larger than 'spark.databricks.adaptive.skewJoin.spillProofPartitionThresholdInBytes'.",""],["spark.databricks.analyzer.tableResolution.metastore.threadpoolSize","5","The threadpool size for fetching relation information from meta store.",""],["spark.databricks.analyzer.tableResolution.threadpoolSize","64","The threadpool size for fetching relation information after the first metastore callis made, e.g., for Delta tables it involves RPCs to S3 for obtaining the table schema.",""],["spark.databricks.catalog.getTablesByName.parallel.enabled","true","Parallelize the Delta table schema resolution when getTablesByName is called.\n      ",""],["spark.databricks.cloudFiles.checkSourceChanged","true","Throws an error if the files loaded by the source seem to belong to a different bucket\nor contianer.\n        ",""],["spark.databricks.cloudFiles.missingMetadataFile.writeNew","false","If the metadata file is missing, then the checkpoint directory is already corrupted.\nThis flag allows user to proceed and write a new metadata file. Writing a new metadata\nfile may change the default options specified in the original metadata file, so we only\nwant user to set this after they understand the consequences.\n        ",""],["spark.databricks.cloudFiles.protocolVersion","<undefined>","A configuration to allow users to upgrade the protocol version of the CloudFileSource.",""],["spark.databricks.cloudFiles.rootSchemaLocation","<undefined>","Provides a default root location for all schemas to be stored for Auto Loader. If this option is not set, the schema location must be explicitly provided in the stream.",""],["spark.databricks.cloudFiles.schemaInference.enabled","false","Whether to enable schema inference in cloud files source",""],["spark.databricks.cloudFiles.sourceMetrics.enabled","true","Whether to report source metrics in cloud files source",""],["spark.databricks.cloudFiles.sqlApi.enabled","false","A feature flag for enabling the SQL API for Auto Loader as a table valued function.",""],["spark.databricks.credentials.assumed.role","<undefined>","The role that will be assumed when trying to access the filesystem.",""],["spark.databricks.delta.alterLocation.bypassSchemaCheck","false","If true, Alter Table Set Location on Delta will go through even if the Delta table in the new location has a different schema from the original Delta table.",""],["spark.databricks.delta.alterTable.rename.enabledOnAWS","false","When true, allow ALTER TABLE ... RENAME TO ... on managed Delta tables on AWS.",""],["spark.databricks.delta.autoCompact.enabled","<undefined>","Whether to compact files after writes made into Delta tables from this session.",""],["spark.databricks.delta.checkLatestSchemaOnRead","true","In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation. This flag enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way.",""],["spark.databricks.delta.checkpoint.async.enabled","<undefined>","Whether checkpoints can be written asynchronously and not block the post-commit hooks.",""],["spark.databricks.delta.commitInfo.enabled","true","Whether to log commit information into the Delta log.",""],["spark.databricks.delta.commitInfo.userMetadata","<undefined>","Arbitrary user-defined metadata to include in CommitInfo. Requires commitInfo.enabled.",""],["spark.databricks.delta.convert.metadataCheck.enabled","true","\nIf enabled, during convert to delta, if there is a difference between the catalog table's\nproperties and the Delta table's configuration, we should error. If disabled, merge\nthe two configurations with the same semantics as update and merge.\n        ",""],["spark.databricks.delta.convert.useMetadataLog","true"," When converting to a Parquet table that was created by Structured Streaming, whether\n  to use the transaction log under `_spark_metadata` as the source of truth for files\n contained in the table.\n        ",""],["spark.databricks.delta.fastQueryPath.dataskipping.checkpointRequired","false","Whether we require a checkpoint file for v2 dataskipping to kick in. If the JSON files are small, we can start kicking in before the first checkpoint is written.",""],["spark.databricks.delta.fastQueryPath.dataskipping.deltaSizeThreshold","10485760b","A size threshold for how large JSON (delta) files can be in the log segment to use DataSkippingReaderV2. If the size is larger than this, we will fallback to dataskipping after LogReplay.",""],["spark.databricks.delta.fastQueryPath.dataskipping.driverCache.enabled","true","A cache of JSON files on the Driver.",""],["spark.databricks.delta.fastQueryPath.dataskipping.enabled","true","Enables the data skipping code path that performs data skipping without performing log replay on Delta's transaction log.",""],["spark.databricks.delta.fastQueryPath.enabled","true","If enabled, analysis and data skipping on Delta tables will go through a fast path that does minimal amount of work, and omits expensive operations like checksum verification on queries.",""],["spark.databricks.delta.formatCheck.cache.enabled","true","If true, we cache checks for whether a path belongs to a Delta table or not.",""],["spark.databricks.delta.formatCheck.enabled","true","If true, checks the specified format and reject reads from / writes to Delta tables using the regular format.",""],["spark.databricks.delta.history.metricsEnabled","true","Enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics.",""],["spark.databricks.delta.lastCommitVersionInSession","<undefined>","The version of the last commit made in the SparkSession for any table.",""],["spark.databricks.delta.optimize.incremental","true","Controls whether the OPTIMIZE command shall operate on \"unoptimized\" data only or just blindly rewrite all data files (matching the given predicate, if any). When enabled, the command's runtime should generally be proportional to the amount of data added since last time the command was run, but this is not a strong guarantee. Also note that this option takes effect no matter if the ZORDER BY clause is specified or not.",""],["spark.databricks.delta.optimizeWrite.enabled","<undefined>","Whether to optimize writes made into Delta tables from this session.",""],["spark.databricks.delta.prepareDeltaScan.parallel.enabled","true","If enabled, fetches the delta index for delta table scans in parallel.",""],["spark.databricks.delta.prepareDeltaScan.threadpool.size","64","The threadpool size for fetching Delta scans in parallel.",""],["spark.databricks.delta.properties.defaults.minReaderVersion","1","The default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.",""],["spark.databricks.delta.properties.defaults.minWriterVersion","2","The default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.",""],["spark.databricks.delta.retentionDurationCheck.enabled","true","Adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting the Delta Log.",""],["spark.databricks.delta.schema.autoMerge.enabled","false","If true, enables schema merging on appends and on overwrites.",""],["spark.databricks.delta.stalenessLimit","0ms","Setting a non-zero time limit will allow you to query the last loaded state of the Delta\ntable without blocking on a table update. You can use this configuration to reduce the\nlatency on queries when up-to-date results are not a requirement. Table updates will be\nscheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources\nfairly with your query. If a table hasn't updated past this time limit, we will block\non a synchronous state update before running the query.\n        ",""],["spark.databricks.delta.vacuum.logging.enabled","<undefined>","Whether to log vacuum information into the Delta transaction log. 'spark.databricks.delta.commitInfo.enabled' should be enabled when using this config.",""],["spark.databricks.delta.vacuum.parallelDelete.enabled","false","Enables parallelizing the deletion of files during a vacuum command. Enabling may result hitting rate limits on some storage backends. When enabled, parallelization is controlled by the default number of shuffle partitions.",""],["spark.databricks.delta.writeChecksumFile.enabled","true","Whether the checksum file can be written.",""],["spark.databricks.execution.arrowCollect.maxBytesPerBatch","131072","When collecting results as ArrowRecordBatches on executors, limit the maximum number of bytes that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit.",""],["spark.databricks.execution.arrowCollect.maxRecordsPerBatch","1000","When collecting results as ArrowRecordBatches on executors, limit the maximum number of records that can be written to a single batch in memory. If set to zero or negative there is no limit.",""],["spark.databricks.hive.metastore.client.pool.enabled","true","When set to true, a pool of hive clients is used",""],["spark.databricks.hive.metastore.client.pool.log.fullStackTrace","true","Whether or not to log the full stack trace when the object is borrowed",""],["spark.databricks.hive.metastore.client.pool.size","5","The size of the hive client pool. Only valid when the client pool is enabled",""],["spark.databricks.hive.metastore.client.pool.waitTime","-1","The max time (ms) a thread waits for a hive client in the pool to become idle",""],["spark.databricks.hive.metastore.connection.maxIdleMillis","60000","The max time a connection is idle before being closed. Keep this number small to reduce the physical connections, but an aggressive value may increase the connection latency.",""],["spark.databricks.io.cache.parquet.enabled","true","Enables automatic caching of Parquet files, if spark.databricks.io.cache.enabled is set to true.",""],["spark.databricks.io.cache.parquet.numWriters","1","The number of asynchronous workers writing Parquet page cache.",""],["spark.databricks.io.cache.prefix","dbfs,s3,hdfs,wasb,adl,abfs,gs,cpfs-","Only paths with the specified scheme prefixes (separated by comma) are cached. If a path doesn't have a scheme, use the default file system URI to decide.",""],["spark.databricks.io.hive.convertMetastoreAvro","false","When set to true, the built-in Avro reader and writer are used to process Avro tables created by using the HiveQL syntax, instead of Hive serde. Note Avro logical types are not supported. Please make sure that there are no logical types in Avro Schema if this flag is enabled.",""],["spark.databricks.io.parquet.verifyChecksumOnWrite.schemes","s3,s3a,abfs,abfss,dbfs","Only paths with the specified schemes (separated by comma) are allowed to enableverification of parquet page checksum on write",""],["spark.databricks.optimizer.adaptive.exchange.targetPartitionSize","67108864b","The target post-exchange input size in bytes of a task.",""],["spark.databricks.optimizer.adaptive.excludedRules","<undefined>","Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have been excluded.",""],["spark.databricks.optimizer.dynamicPartitionPruning","true","When true, we will generate predicate for partition column when it's used as join key",""],["spark.databricks.optimizer.rangeJoin.binSize","0.0","Binsize to use with RangeJoin",""],["spark.databricks.pyspark.pythonUdfsOnly","false","If true, allow only Python UDFs to be executed",""],["spark.databricks.service.client.session.cache.size","20","The maximum size of the session cache for Spark Service client.",""],["spark.databricks.service.dbutils.fs.parallel.ls.threadPoolSize","20","Thread pool size for the parallel recursive listing done on the driver for parallel dbutils.fs.[cp/mv/rm] operations.",""],["spark.databricks.service.dbutils.fs.parallel.ls.timeoutSeconds","7200","Timeout in seconds for the parallel recursive listing done on the driver for parallel dbutils.fs.[cp/mv/rm] operations.",""],["spark.databricks.sql.files.prorateMaxPartitionBytes.ubound","1073741824b","The upper bound for proration of ConfigEntry(key=spark.sql.files.maxPartitionBytes, defaultValue=128MB, doc=The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC., public=true, version=2.0.0).",""],["spark.databricks.sql.minBucketsForBucketedJoin","<undefined>","The minimum number of buckets to perform bucketed join with a bucketed table and a non-bucketed one, to avoid shuffling the bucketed table. If not set, the default value is 'ConfigEntry(key=spark.sql.shuffle.partitions, defaultValue=200, doc=The default number of partitions to use when shuffling data for joins or aggregations. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location., public=true, version=1.1.0)'.",""],["spark.databricks.sql.optimizer.maxAliasReplacementProjectListSize","100","Alias replacement will be skipped if the number of aliased expressions in a Project, Aggregate or Window plan exceeds this threshold, which can potentially lead to missing some optimization opportunities but will not affect correctness.",""],["spark.databricks.streaming.schemaEvolution.enabled","true","Whether structured streaming supports schema evolution. If enabled, mirco-batch execution can detect schema change in a batch and triggers schema evolution on the source schema.",""],["spark.databricks.thriftserver.getTables.threadPool.size","64","The maximum number of threads in the pool used to handle listing of tables of each database within Thriftserver's GetTables operation",""],["spark.sql.adaptive.advisoryPartitionSizeInBytes","67108864b","The advisory size in bytes of the shuffle partition during adaptive optimization (when 'spark.sql.adaptive.enabled' is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.","3.0.0"],["spark.sql.adaptive.coalescePartitions.enabled","true","When true and 'spark.sql.adaptive.enabled' is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks.","3.0.0"],["spark.sql.adaptive.coalescePartitions.initialPartitionNum","<undefined>","The initial number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.","3.0.0"],["spark.sql.adaptive.coalescePartitions.minPartitionNum","<undefined>","The suggested (not guaranteed) minimum number of shuffle partitions after coalescing. If not set, the default value is 2 times the default parallelism of the Spark cluster. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.","3.0.0"],["spark.sql.adaptive.coalescePartitions.minPartitionSize","1MB","The minimum size of shuffle partitions after coalescing if 'spark.sql.adaptive.coalescePartitions.minPartitionNum' is not set. When 'spark.sql.adaptive.coalescePartitions.minPartitionNum' is not set, the minimum number of shuffle partitions after coalescing will fall back to 2 times the default parallelism of the Spark cluster, sometimes causing partition sizes to be too small. This config aims to prevent this situation. Note that this config does not take effect if 'spark.sql.adaptive.coalescePartitions.minPartitionNum' is set by the user.","3.1.0"],["spark.sql.adaptive.enabled","true","When true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.","1.6.0"],["spark.sql.adaptive.localShuffleReader.enabled","true","When true and 'spark.sql.adaptive.enabled' is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.","3.0.0"],["spark.sql.adaptive.optimizer.excludedRules","<undefined>","Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.","3.1.0"],["spark.sql.adaptive.skewJoin.enabled","true","When true and 'spark.sql.adaptive.enabled' is true, Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if needed) skewed partitions.","3.0.0"],["spark.sql.adaptive.skewJoin.skewedPartitionFactor","5","A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'","3.0.0"],["spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes","256MB","A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Ideally this config should be set larger than 'spark.sql.adaptive.advisoryPartitionSizeInBytes'.","3.0.0"],["spark.sql.ansi.enabled","false","When true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results when the inputs to a SQL operator/function are invalid.For full details of this dialect, you can find them in the section \"ANSI Compliance\" of Spark's documentation. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL's style","3.0.0"],["spark.sql.autoBroadcastJoinThreshold","10MB","Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.  By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command `ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan` has been run, and file-based data source tables where the statistics are computed directly on the files of data.","1.1.0"],["spark.sql.avro.compression.codec","snappy","Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.","2.4.0"],["spark.sql.avro.deflate.level","-1","Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.","2.4.0"],["spark.sql.avro.filterPushdown.enabled","true","When true, enable filter pushdown to Avro datasource.","3.1.0"],["spark.sql.broadcastTimeout","300","Timeout in seconds for the broadcast wait time in broadcast joins.","1.3.0"],["spark.sql.bucketing.coalesceBucketsInJoin.enabled","false","When true, if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.","3.1.0"],["spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio","4","The ratio of the number of two buckets being coalesced should be less than or equal to this value for bucket coalescing to be applied. This configuration only has an effect when 'spark.sql.bucketing.coalesceBucketsInJoin.enabled' is set to true.","3.1.0"],["spark.sql.cache.serializer","org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer","The name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.","3.1.0"],["spark.sql.catalog.spark_catalog","<undefined>","A catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog. This catalog shares its identifier namespace with the spark_catalog and must be consistent with it; for example, if a table can be loaded by the spark_catalog, this catalog must also return the table metadata. To delegate operations to the spark_catalog, implementations can extend 'CatalogExtension'.","3.0.0"],["spark.sql.cbo.enabled","true","Enables CBO for estimation of plan statistics when set true.","2.2.0"],["spark.sql.cbo.joinReorder.dp.star.filter","false","Applies star-join filter heuristics to cost based join enumeration.","2.2.0"],["spark.sql.cbo.joinReorder.dp.threshold","12","The maximum number of joined nodes allowed in the dynamic programming algorithm.","2.2.0"],["spark.sql.cbo.joinReorder.enabled","true","Enables join reorder in CBO.","2.2.0"],["spark.sql.cbo.planStats.enabled","false","When true, the logical plan will fetch row counts and column statistics from catalog.","3.0.0"],["spark.sql.cbo.starSchemaDetection","false","When true, it enables join reordering based on star schema detection. ","2.2.0"],["spark.sql.columnNameOfCorruptRecord","_corrupt_record","The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.","1.2.0"],["spark.sql.csv.filterPushdown.enabled","true","When true, enable filter pushdown to CSV datasource.","3.0.0"],["spark.sql.datetime.java8API.enabled","false","If the configuration property is set to true, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp and java.sql.Date are used for the same purpose.","3.0.0"],["spark.sql.debug.maxToStringFields","25","Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a \"... N more fields\" placeholder.","3.0.0"],["spark.sql.defaultCatalog","spark_catalog","Name of the default catalog. This will be the current catalog if users have not explicitly set the current catalog yet.","3.0.0"],["spark.sql.event.truncate.length","2147483647","Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.","3.0.0"],["spark.sql.execution.arrow.enabled","false","(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.enabled'.)","2.3.0"],["spark.sql.execution.arrow.fallback.enabled","true","(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.fallback.enabled'.)","2.4.0"],["spark.sql.execution.arrow.maxRecordsPerBatch","10000","When using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit.","2.3.0"],["spark.sql.execution.arrow.pyspark.enabled","false","When true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to: 1. pyspark.sql.DataFrame.toPandas 2. pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame The following data types are unsupported: ArrayType of TimestampType, and nested StructType.","3.0.0"],["spark.sql.execution.arrow.pyspark.fallback.enabled","true","When true, optimizations enabled by 'spark.sql.execution.arrow.pyspark.enabled' will fallback automatically to non-optimized implementations if an error occurs.","3.0.0"],["spark.sql.execution.arrow.sparkr.enabled","false","When true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to: 1. createDataFrame when its input is an R DataFrame 2. collect 3. dapply 4. gapply The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.","3.0.0"],["spark.sql.execution.pandas.udf.buffer.size","65536","Same as `spark.buffer.size` but only applies to Pandas UDF executions. If it is not set, the fallback is `spark.buffer.size`. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.","3.0.0"],["spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled","true","When true, the traceback from Python UDFs is simplified. It hides the Python worker, (de)serialization, etc from PySpark in tracebacks, and only shows the exception messages from UDFs. Note that this works only with CPython 3.7+.","3.1.0"],["spark.sql.extensions","<undefined>","A comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.","2.2.0"],["spark.sql.files.ignoreCorruptFiles","false","Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.","2.1.1"],["spark.sql.files.ignoreMissingFiles","false","Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.","2.3.0"],["spark.sql.files.maxPartitionBytes","128MB","The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.","2.0.0"],["spark.sql.files.maxRecordsPerFile","0","Maximum number of records to write out to a single file. If this value is zero or negative, there is no limit.","2.2.0"],["spark.sql.files.minPartitionNum","<undefined>","The suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is `spark.default.parallelism`. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.","3.1.0"],["spark.sql.function.concatBinaryAsString","false","When this option is set to false and all inputs are binary, `functions.concat` returns an output as binary. Otherwise, it returns as a string.","2.3.0"],["spark.sql.function.eltOutputAsString","false","When this option is set to false and all inputs are binary, `elt` returns an output as binary. Otherwise, it returns as a string.","2.3.0"],["spark.sql.groupByAliases","true","This configuration is only effective when ANSI mode is disabled. When it is true and spark.sql.ansi.enabled is false, aliases in a select list can be used in group by clauses. Otherwise, an analysis exception is thrown in the case.","2.2.0"],["spark.sql.groupByOrdinal","true","When true, the ordinal numbers in group by clauses are treated as the position in the select list. When false, the ordinal numbers are ignored.","2.0.0"],["spark.sql.hive.convertInsertingPartitionedTable","true","When set to true, and `spark.sql.hive.convertMetastoreParquet` or `spark.sql.hive.convertMetastoreOrc` is true, the built-in ORC/Parquet writer is usedto process inserting into partitioned ORC/Parquet tables created by using the HiveSQL syntax.","3.0.0"],["spark.sql.hive.convertMetastoreCtas","true","When set to true,  Spark will try to use built-in data source writer instead of Hive serde in CTAS. This flag is effective only if `spark.sql.hive.convertMetastoreParquet` or `spark.sql.hive.convertMetastoreOrc` is enabled respectively for Parquet and ORC formats","3.0.0"],["spark.sql.hive.convertMetastoreOrc","true","When set to true, the built-in ORC reader and writer are used to process ORC tables created by using the HiveQL syntax, instead of Hive serde.","2.0.0"],["spark.sql.hive.convertMetastoreParquet","true","When set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde.","1.1.1"],["spark.sql.hive.convertMetastoreParquet.mergeSchema","false","When true, also tries to merge possibly different but compatible Parquet schemas in different Parquet data files. This configuration is only effective when \"spark.sql.hive.convertMetastoreParquet\" is true.","1.3.1"],["spark.sql.hive.filesourcePartitionFileCacheSize","262144000","When nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.","2.1.1"],["spark.sql.hive.manageFilesourcePartitions","true","When true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning.","2.1.1"],["spark.sql.hive.metastore.barrierPrefixes","","A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).","1.4.0"],["spark.sql.hive.metastore.jars","/databricks/hive/*","\n Location of the jars that should be used to instantiate the HiveMetastoreClient.\n This property can be one of four options:\n 1. \"builtin\"\n   Use Hive 2.3.7, which is bundled with the Spark assembly when\n   <code>-Phive</code> is enabled. When this option is chosen,\n   <code>spark.sql.hive.metastore.version</code> must be either\n   <code>2.3.7</code> or not defined.\n 2. \"maven\"\n   Use Hive jars of specified version downloaded from Maven repositories.\n 3. \"path\"\n   Use Hive jars configured by `spark.sql.hive.metastore.jars.path`\n   in comma separated format. Support both local or remote paths.The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n 4. A classpath in the standard format for both Hive and Hadoop. The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n      ","1.4.0"],["spark.sql.hive.metastore.jars.path","","\n Comma-separated paths of the jars that used to instantiate the HiveMetastoreClient.\n This configuration is useful only when `{ConfigEntry(key=spark.sql.hive.metastore.jars, defaultValue=builtin, doc=\n Location of the jars that should be used to instantiate the HiveMetastoreClient.\n This property can be one of four options:\n 1. \"builtin\"\n   Use Hive 2.3.7, which is bundled with the Spark assembly when\n   <code>-Phive</code> is enabled. When this option is chosen,\n   <code>spark.sql.hive.metastore.version</code> must be either\n   <code>2.3.7</code> or not defined.\n 2. \"maven\"\n   Use Hive jars of specified version downloaded from Maven repositories.\n 3. \"path\"\n   Use Hive jars configured by `spark.sql.hive.metastore.jars.path`\n   in comma separated format. Support both local or remote paths.The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n 4. A classpath in the standard format for both Hive and Hadoop. The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n      , public=true, version=1.4.0).key}` is set as `path`.\n The paths can be any of the following format:\n 1. file://path/to/jar/foo.jar\n 2. hdfs://nameservice/path/to/jar/foo.jar\n 3. /path/to/jar/ (path without URI scheme follow conf `fs.defaultFS`'s URI schema)\n 4. [http/https/ftp]://path/to/jar/foo.jar\n Note that 1, 2, and 3 support wildcard. For example:\n 1. file://path/to/jar/*,file://path2/to/jar/*/*.jar\n 2. hdfs://nameservice/path/to/jar/*,hdfs://nameservice2/path/to/jar/*/*.jar\n      ","3.1.0"],["spark.sql.hive.metastore.sharedPrefixes","org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks","A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.","1.4.0"],["spark.sql.hive.metastore.version","0.13.0","Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>.","1.4.0"],["spark.sql.hive.metastorePartitionPruning","true","When true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier. This only affects Hive tables not converted to filesource relations (see HiveUtils.CONVERT_METASTORE_PARQUET and HiveUtils.CONVERT_METASTORE_ORC for more information).","1.5.0"],["spark.sql.hive.thriftServer.async","true","When set to true, Hive Thrift server executes SQL queries in an asynchronous way.","1.5.0"],["spark.sql.hive.thriftServer.singleSession","false","When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.","1.6.0"],["spark.sql.hive.verifyPartitionPath","false","When true, check all the partition paths under the table's root directory when reading data stored in HDFS. This configuration will be deprecated in the future releases and replaced by spark.files.ignoreMissingFiles.","1.4.0"],["spark.sql.hive.version","0.13.0","deprecated, please use spark.sql.hive.metastore.version to get the Hive version in Spark.","1.1.1"],["spark.sql.inMemoryColumnarStorage.batchSize","10000","Controls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.","1.1.1"],["spark.sql.inMemoryColumnarStorage.compressed","true","When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.","1.0.1"],["spark.sql.inMemoryColumnarStorage.enableVectorizedReader","true","Enables vectorized reader for columnar caching.","2.3.1"],["spark.sql.json.filterPushdown.enabled","true","When true, enable filter pushdown to JSON datasource.","3.1.0"],["spark.sql.jsonGenerator.ignoreNullFields","true","Whether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.","3.0.0"],["spark.sql.legacy.allowHashOnMapType","false","When set to true, hash expressions can be applied on elements of MapType. Otherwise, an analysis exception will be thrown.","3.0.0"],["spark.sql.legacy.sessionInitWithConfigDefaults","false","Flag to revert to legacy behavior where a cloned SparkSession receives SparkConf defaults, dropping any overrides in its parent SparkSession.","3.0.0"],["spark.sql.mapKeyDedupPolicy","EXCEPTION","The policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.","3.0.0"],["spark.sql.maven.additionalRemoteRepositories","https://maven-central.storage-download.googleapis.com/maven2/","A comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.","3.0.0"],["spark.sql.maxMetadataStringLength","100","Maximum number of characters to output for a metadata string. e.g. file location in `DataSourceScanExec`, every value will be abbreviated if exceed length.","3.1.0"],["spark.sql.maxPlanStringLength","2147483632","Maximum number of characters to output for a plan string.  If the plan is longer, further output will be truncated.  The default setting always generates a full plan.  Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.","3.0.0"],["spark.sql.metadataCacheTTLSeconds","-1000ms","Time-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (> 0). It also requires setting 'spark.sql.catalogImplementation' to `hive`, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' > 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to `true` to be applied to the partition file metadata cache.","3.1.0"],["spark.sql.optimizer.dynamicPartitionPruning.enabled","true","When true, we will generate predicate for partition column when it's used as join key","3.0.0"],["spark.sql.optimizer.enableJsonExpressionOptimization","true","Whether to optimize JSON expressions in SQL optimizer. It includes pruning unnecessary columns from from_json, simplifing from_json + to_json, to_json + named_struct(from_json.col1, from_json.col2, ....).","3.1.0"],["spark.sql.optimizer.excludedRules","<undefined>","Configures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded.","2.4.0"],["spark.sql.orc.columnarReaderBatchSize","4096","The number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.","2.4.0"],["spark.sql.orc.compression.codec","snappy","Sets the compression codec used when writing ORC files. If either `compression` or `orc.compress` is specified in the table-specific options/properties, the precedence would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.Acceptable values include: none, uncompressed, snappy, zlib, lzo.","2.3.0"],["spark.sql.orc.enableVectorizedReader","true","Enables vectorized orc decoding.","2.3.0"],["spark.sql.orc.filterPushdown","true","When true, enable filter pushdown for ORC files.","1.4.0"],["spark.sql.orc.mergeSchema","false","When true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.","3.0.0"],["spark.sql.orderByOrdinal","true","When true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.","2.0.0"],["spark.sql.parquet.binaryAsString","false","Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.","1.1.1"],["spark.sql.parquet.columnarReaderBatchSize","4096","The number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.","2.4.0"],["spark.sql.parquet.compression.codec","snappy","Sets the compression codec used when writing Parquet files. If either `compression` or `parquet.compression` is specified in the table-specific options/properties, the precedence would be `compression`, `parquet.compression`, `spark.sql.parquet.compression.codec`. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, zstd.","1.1.1"],["spark.sql.parquet.enableVectorizedReader","true","Enables vectorized parquet decoding.","2.0.0"],["spark.sql.parquet.filterPushdown","true","Enables Parquet filter push-down optimization when set to true.","1.2.0"],["spark.sql.parquet.int96AsTimestamp","true","Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.","1.3.0"],["spark.sql.parquet.int96TimestampConversion","false","This controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive & Spark. Note that this config is ignored when spark.databricks.io.parquet.fastreader.enabled is set to true.","2.3.0"],["spark.sql.parquet.mergeSchema","false","When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.","1.5.0"],["spark.sql.parquet.outputTimestampType","INT96","Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.","2.3.0"],["spark.sql.parquet.recordLevelFilter.enabled","false","If true, enables Parquet's native record-level filtering using the pushed down filters. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting 'spark.sql.parquet.enableVectorizedReader' to false.","2.3.0"],["spark.sql.parquet.respectSummaryFiles","false","When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.","1.5.0"],["spark.sql.parquet.writeLegacyFormat","false","If true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet's fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.","1.6.0"],["spark.sql.parser.quotedRegexColumnNames","false","When true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.","2.3.0"],["spark.sql.pivotMaxValues","10000","When doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.","1.6.0"],["spark.sql.pyspark.jvmStacktrace.enabled","false","When true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled and hides JVM stacktrace and shows a Python-friendly exception only.","3.0.0"],["spark.sql.queryExecutionListeners","<undefined>","List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.","2.3.0"],["spark.sql.redaction.options.regex","(?i)url","Regex to decide which keys in a Spark SQL command's options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.","2.2.2"],["spark.sql.redaction.string.regex","(?<![A-Za-z0-9/+=])([A-Za-z0-9/+=]|%2F|%2B|%3D){40}(?=@)","Regex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from `spark.redaction.string.regex` is used.","2.3.0"],["spark.sql.repl.eagerEval.enabled","false","Enables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is supported in PySpark and SparkR. In PySpark, for the notebooks like Jupyter, the HTML table (generated by _repr_html_) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned outputs are showed similar to R data.frame would.","2.4.0"],["spark.sql.repl.eagerEval.maxNumRows","20","The max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).","2.4.0"],["spark.sql.repl.eagerEval.truncate","20","The max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.","2.4.0"],["spark.sql.session.timeZone","Etc/UTC","The ID of session local timezone in the format of either region-based zone IDs or zone offsets. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm' or '(+|-)HH:mm:ss', e.g '-08', '+01:00' or '-13:33:33'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'. Other short names are not recommended to use because they can be ambiguous.","2.2.0"],["spark.sql.shuffle.partitions","200","The default number of partitions to use when shuffling data for joins or aggregations. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location.","1.1.0"],["spark.sql.sources.bucketing.autoBucketedScan.enabled","true","When true, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan. Note when 'spark.sql.sources.bucketing.enabled' is set to false, this configuration does not take any effect.","3.1.0"],["spark.sql.sources.bucketing.enabled","true","When false, we will treat bucketed table as normal table","2.0.0"],["spark.sql.sources.bucketing.maxBuckets","100000","The maximum number of buckets allowed.","2.4.0"],["spark.sql.sources.default","delta","The default data source to use in input/output.","1.3.0"],["spark.sql.sources.parallelPartitionDiscovery.threshold","32","The maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.","1.5.0"],["spark.sql.sources.partitionColumnTypeInference.enabled","true","When true, automatically infer the data types for partitioned columns.","1.5.0"],["spark.sql.sources.partitionOverwriteMode","STATIC","When INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn't delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(\"partitionOverwriteMode\", \"dynamic\").save(path).","2.3.0"],["spark.sql.statistics.fallBackToHdfs","false","When true, it will fall back to HDFS if the table statistics are not available from table metadata. This is useful in determining if a table is small enough to use broadcast joins. This flag is effective only for non-partitioned Hive tables. For non-partitioned data source tables, it will be automatically recalculated if table statistics are not available. For partitioned data source and partitioned Hive tables, It is 'spark.sql.defaultSizeInBytes' if table statistics are not available.","2.0.0"],["spark.sql.statistics.histogram.enabled","false","Generates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.","2.3.0"],["spark.sql.statistics.size.autoUpdate.enabled","false","Enables automatic update for table size once table's data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.","2.3.0"],["spark.sql.storeAssignmentPolicy","ANSI","When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting `string` to `int` or `double` to `boolean`. With legacy policy, Spark allows the type coercion as long as it is a valid `Cast`, which is very loose. e.g. converting `string` to `int` or `double` to `boolean` is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting `double` to `int` or `decimal` to `double` is not allowed.","3.0.0"],["spark.sql.streaming.checkpointLocation","<undefined>","The default location for storing checkpoint data for streaming queries.","2.0.0"],["spark.sql.streaming.continuous.epochBacklogQueueSize","10000","The max number of entries to be stored in queue to wait for late epochs. If this parameter is exceeded by the size of the queue, stream will stop with an error.","3.0.0"],["spark.sql.streaming.disabledV2Writers","","A comma-separated list of fully qualified data source register class names for which StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.","2.3.1"],["spark.sql.streaming.fileSource.cleaner.numThreads","1","Number of threads used in the file source completed file cleaner.","3.0.0"],["spark.sql.streaming.forceDeleteTempCheckpointLocation","false","When true, enable temporary checkpoint locations force delete.","3.0.0"],["spark.sql.streaming.metricsEnabled","false","Whether Dropwizard/Codahale metrics will be reported for active streaming queries.","2.0.2"],["spark.sql.streaming.multipleWatermarkPolicy","min","Policy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is 'min' which chooses the minimum watermark reported across multiple operators. Other alternative value is 'max' which chooses the maximum across multiple operators. Note: This configuration cannot be changed between query restarts from the same checkpoint location.","2.4.0"],["spark.sql.streaming.noDataMicroBatches.enabled","true","Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.","2.4.1"],["spark.sql.streaming.numRecentProgressUpdates","100","The number of progress updates to retain for a streaming query","2.1.1"],["spark.sql.streaming.stateStore.stateSchemaCheck","true","When true, Spark will validate the state schema against schema on existing state and fail query if it's incompatible.","3.1.0"],["spark.sql.streaming.stopActiveRunOnRestart","true","Running multiple runs of the same streaming query concurrently is not supported. If we find a concurrent active run for a streaming query (in the same or different SparkSessions on the same cluster) and this flag is true, we will stop the old streaming query run to start the new one.","3.0.0"],["spark.sql.streaming.stopTimeout","15s","How long to wait in milliseconds for the streaming execution thread to stop when calling the streaming query's stop() method. 0 or negative values wait indefinitely.","3.0.0"],["spark.sql.streaming.streamingQueryListeners","<undefined>","List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.","2.4.0"],["spark.sql.streaming.ui.enabled","true","Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.","3.0.0"],["spark.sql.streaming.ui.retainedProgressUpdates","100","The number of progress updates to retain for a streaming query for Structured Streaming UI.","3.0.0"],["spark.sql.streaming.ui.retainedQueries","100","The number of inactive queries to retain for Structured Streaming UI.","3.0.0"],["spark.sql.thriftServer.queryTimeout","0ms","Set a query duration timeout in seconds in Thrift Server. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. If timeout values are set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller than this configuration value, they take precedence.","3.1.0"],["spark.sql.thriftserver.scheduler.pool","<undefined>","Set a Fair Scheduler pool for a JDBC client session.","1.1.1"],["spark.sql.thriftserver.ui.retainedSessions","200","The number of SQL client sessions kept in the JDBC/ODBC web UI history.","1.4.0"],["spark.sql.thriftserver.ui.retainedStatements","200","The number of SQL statements kept in the JDBC/ODBC web UI history.","1.4.0"],["spark.sql.ui.explainMode","formatted","Configures the query explain mode used in the Spark SQL UI. The value can be 'simple', 'extended', 'codegen', 'cost', or 'formatted'. The default value is 'formatted'.","3.1.0"],["spark.sql.ui.retainedExecutions","1000","Number of executions to retain in the Spark UI.","1.5.0"],["spark.sql.variable.substitute","true","This enables substitution using syntax like `${var}`, `${system:var}`, and `${env:var}`.","2.0.0"],["spark.sql.warehouse.dir","/user/hive/warehouse","The default location for managed databases and tables.","2.0.0"],["spark.thriftserver.arrowBasedRowSet.maxBytesPerFetchLimit","10485760","Set a hard limit on the size of the Arrow-based row set constructed per each fetch request, even if the client requests more via TFetchResultsReq.maxBytes.If set to zero or negative there is no limit.",""],["spark.thriftserver.arrowBasedRowSet.timestampAsString","true","When true, convert timestamp columns to strings",""],["spark.thriftserver.cloudStoreBasedRowSet.enabled","true","When true, use the CloudStore-based RowSet in Thrift. Otherwise, fall-back to the RowSet resolved by spark.thriftserver.arrowBasedRowSet.enabled.",""],["spark.thriftserver.cloudStoreBasedRowSet.executor.cloudUploadThreshold","1048576b","If the result size is below this threshold, arrow batches will returned instead  of cloud files",""],["spark.thriftserver.cloudStoreBasedRowSet.maxBytesPerFetchLimit","1073741824","Set a hard limit on the size of the cloud-based row set constructed per each fetch request. If set to zero or negative there is no limit.",""],["spark.thriftserver.cloudStoreBasedRowSet.smallResultsOptimization.enabled","true","When true, smaller results sets will be returned as arrow batches instead of cloud files.",""],["spark.thriftserver.cloudfetch.enabled","true","When true, use the CloudStore-based RowSet in Thrift. Otherwise, fall-back to the RowSet resolved by spark.thriftserver.arrowBasedRowSet.enabled.",""]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"key","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"string\"","metadata":"{}"},{"name":"meaning","type":"\"string\"","metadata":"{}"},{"name":"Since version","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th><th>meaning</th><th>Since version</th></tr></thead><tbody><tr><td>spark.databricks.adaptive.autoBroadcastJoinThreshold</td><td>31457280b</td><td>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join when adaptive execution is enabled. By setting this value to -1 broadcasting can be disabled.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.aggregateRatio</td><td>0.1</td><td>The aggregate output/input row ratio employed to estimate aggregate output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.aggregateSpillFactor</td><td>0.1</td><td>The ratio of spilling by aggregate compared to sort of the same input size used in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.equalityFilterSelectivity</td><td>0.1</td><td>The filter selectivity of a single equality predicate employed to estimate filter output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.exchangeSpillFactor</td><td>0.1</td><td>The ratio of spilling by exchange compared to sort of the same input size used in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.filterSelectivity</td><td>0.5</td><td>The filter selectivity of a single non-equality predicate employed to estimate filter output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.joinFactor</td><td>1.0</td><td>The co-efficient used by join output size estimation in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.joinSpillFactor</td><td>1.0</td><td>The ratio of spilling by shuffled hash join compared to sort of the same input size used in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.maxPartitionNumber</td><td>20480</td><td>The maximum initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.minFilterRatio</td><td>0.01</td><td>The minimum output/input row ratio employed to estimate filter output size in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.minPartitionNumber</td><td><undefined></td><td>The minimum initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.preshufflePartitionSizeInBytes</td><td>128MB</td><td>The per partition size used to calculate the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true.</td><td></td></tr><tr><td>spark.databricks.adaptive.autoOptimizeShuffle.useFileSize</td><td>true</td><td>When true, file sizes will be used to estimate the sizes of input relations in the calculation of the initial shuffle partition number when spark.databricks.adaptive.autoOptimizeShuffle.enabled is true; otherwise, the catalog stats will be used for the estimate instead.</td><td></td></tr><tr><td>spark.databricks.adaptive.skewJoin.spillProofPartitionThresholdInBytes</td><td>1GB</td><td>When 'spark.databricks.adaptive.skewJoin.spillProof.enabled' is true, a partition is considered as skewed if its size in bytes is larger than this threshold regardless of other skew join conditions.</td><td></td></tr><tr><td>spark.databricks.adaptive.skewJoin.spillProofTargetPartitionSizeInBytes</td><td>512MB</td><td>The target partition size in bytes for splitting skew partitions larger than 'spark.databricks.adaptive.skewJoin.spillProofPartitionThresholdInBytes'.</td><td></td></tr><tr><td>spark.databricks.analyzer.tableResolution.metastore.threadpoolSize</td><td>5</td><td>The threadpool size for fetching relation information from meta store.</td><td></td></tr><tr><td>spark.databricks.analyzer.tableResolution.threadpoolSize</td><td>64</td><td>The threadpool size for fetching relation information after the first metastore callis made, e.g., for Delta tables it involves RPCs to S3 for obtaining the table schema.</td><td></td></tr><tr><td>spark.databricks.catalog.getTablesByName.parallel.enabled</td><td>true</td><td>Parallelize the Delta table schema resolution when getTablesByName is called.\n      </td><td></td></tr><tr><td>spark.databricks.cloudFiles.checkSourceChanged</td><td>true</td><td>Throws an error if the files loaded by the source seem to belong to a different bucket\nor contianer.\n        </td><td></td></tr><tr><td>spark.databricks.cloudFiles.missingMetadataFile.writeNew</td><td>false</td><td>If the metadata file is missing, then the checkpoint directory is already corrupted.\nThis flag allows user to proceed and write a new metadata file. Writing a new metadata\nfile may change the default options specified in the original metadata file, so we only\nwant user to set this after they understand the consequences.\n        </td><td></td></tr><tr><td>spark.databricks.cloudFiles.protocolVersion</td><td><undefined></td><td>A configuration to allow users to upgrade the protocol version of the CloudFileSource.</td><td></td></tr><tr><td>spark.databricks.cloudFiles.rootSchemaLocation</td><td><undefined></td><td>Provides a default root location for all schemas to be stored for Auto Loader. If this option is not set, the schema location must be explicitly provided in the stream.</td><td></td></tr><tr><td>spark.databricks.cloudFiles.schemaInference.enabled</td><td>false</td><td>Whether to enable schema inference in cloud files source</td><td></td></tr><tr><td>spark.databricks.cloudFiles.sourceMetrics.enabled</td><td>true</td><td>Whether to report source metrics in cloud files source</td><td></td></tr><tr><td>spark.databricks.cloudFiles.sqlApi.enabled</td><td>false</td><td>A feature flag for enabling the SQL API for Auto Loader as a table valued function.</td><td></td></tr><tr><td>spark.databricks.credentials.assumed.role</td><td><undefined></td><td>The role that will be assumed when trying to access the filesystem.</td><td></td></tr><tr><td>spark.databricks.delta.alterLocation.bypassSchemaCheck</td><td>false</td><td>If true, Alter Table Set Location on Delta will go through even if the Delta table in the new location has a different schema from the original Delta table.</td><td></td></tr><tr><td>spark.databricks.delta.alterTable.rename.enabledOnAWS</td><td>false</td><td>When true, allow ALTER TABLE ... RENAME TO ... on managed Delta tables on AWS.</td><td></td></tr><tr><td>spark.databricks.delta.autoCompact.enabled</td><td><undefined></td><td>Whether to compact files after writes made into Delta tables from this session.</td><td></td></tr><tr><td>spark.databricks.delta.checkLatestSchemaOnRead</td><td>true</td><td>In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation. This flag enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way.</td><td></td></tr><tr><td>spark.databricks.delta.checkpoint.async.enabled</td><td><undefined></td><td>Whether checkpoints can be written asynchronously and not block the post-commit hooks.</td><td></td></tr><tr><td>spark.databricks.delta.commitInfo.enabled</td><td>true</td><td>Whether to log commit information into the Delta log.</td><td></td></tr><tr><td>spark.databricks.delta.commitInfo.userMetadata</td><td><undefined></td><td>Arbitrary user-defined metadata to include in CommitInfo. Requires commitInfo.enabled.</td><td></td></tr><tr><td>spark.databricks.delta.convert.metadataCheck.enabled</td><td>true</td><td>\nIf enabled, during convert to delta, if there is a difference between the catalog table's\nproperties and the Delta table's configuration, we should error. If disabled, merge\nthe two configurations with the same semantics as update and merge.\n        </td><td></td></tr><tr><td>spark.databricks.delta.convert.useMetadataLog</td><td>true</td><td> When converting to a Parquet table that was created by Structured Streaming, whether\n  to use the transaction log under `_spark_metadata` as the source of truth for files\n contained in the table.\n        </td><td></td></tr><tr><td>spark.databricks.delta.fastQueryPath.dataskipping.checkpointRequired</td><td>false</td><td>Whether we require a checkpoint file for v2 dataskipping to kick in. If the JSON files are small, we can start kicking in before the first checkpoint is written.</td><td></td></tr><tr><td>spark.databricks.delta.fastQueryPath.dataskipping.deltaSizeThreshold</td><td>10485760b</td><td>A size threshold for how large JSON (delta) files can be in the log segment to use DataSkippingReaderV2. If the size is larger than this, we will fallback to dataskipping after LogReplay.</td><td></td></tr><tr><td>spark.databricks.delta.fastQueryPath.dataskipping.driverCache.enabled</td><td>true</td><td>A cache of JSON files on the Driver.</td><td></td></tr><tr><td>spark.databricks.delta.fastQueryPath.dataskipping.enabled</td><td>true</td><td>Enables the data skipping code path that performs data skipping without performing log replay on Delta's transaction log.</td><td></td></tr><tr><td>spark.databricks.delta.fastQueryPath.enabled</td><td>true</td><td>If enabled, analysis and data skipping on Delta tables will go through a fast path that does minimal amount of work, and omits expensive operations like checksum verification on queries.</td><td></td></tr><tr><td>spark.databricks.delta.formatCheck.cache.enabled</td><td>true</td><td>If true, we cache checks for whether a path belongs to a Delta table or not.</td><td></td></tr><tr><td>spark.databricks.delta.formatCheck.enabled</td><td>true</td><td>If true, checks the specified format and reject reads from / writes to Delta tables using the regular format.</td><td></td></tr><tr><td>spark.databricks.delta.history.metricsEnabled</td><td>true</td><td>Enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics.</td><td></td></tr><tr><td>spark.databricks.delta.lastCommitVersionInSession</td><td><undefined></td><td>The version of the last commit made in the SparkSession for any table.</td><td></td></tr><tr><td>spark.databricks.delta.optimize.incremental</td><td>true</td><td>Controls whether the OPTIMIZE command shall operate on \"unoptimized\" data only or just blindly rewrite all data files (matching the given predicate, if any). When enabled, the command's runtime should generally be proportional to the amount of data added since last time the command was run, but this is not a strong guarantee. Also note that this option takes effect no matter if the ZORDER BY clause is specified or not.</td><td></td></tr><tr><td>spark.databricks.delta.optimizeWrite.enabled</td><td><undefined></td><td>Whether to optimize writes made into Delta tables from this session.</td><td></td></tr><tr><td>spark.databricks.delta.prepareDeltaScan.parallel.enabled</td><td>true</td><td>If enabled, fetches the delta index for delta table scans in parallel.</td><td></td></tr><tr><td>spark.databricks.delta.prepareDeltaScan.threadpool.size</td><td>64</td><td>The threadpool size for fetching Delta scans in parallel.</td><td></td></tr><tr><td>spark.databricks.delta.properties.defaults.minReaderVersion</td><td>1</td><td>The default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.</td><td></td></tr><tr><td>spark.databricks.delta.properties.defaults.minWriterVersion</td><td>2</td><td>The default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.</td><td></td></tr><tr><td>spark.databricks.delta.retentionDurationCheck.enabled</td><td>true</td><td>Adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting the Delta Log.</td><td></td></tr><tr><td>spark.databricks.delta.schema.autoMerge.enabled</td><td>false</td><td>If true, enables schema merging on appends and on overwrites.</td><td></td></tr><tr><td>spark.databricks.delta.stalenessLimit</td><td>0ms</td><td>Setting a non-zero time limit will allow you to query the last loaded state of the Delta\ntable without blocking on a table update. You can use this configuration to reduce the\nlatency on queries when up-to-date results are not a requirement. Table updates will be\nscheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources\nfairly with your query. If a table hasn't updated past this time limit, we will block\non a synchronous state update before running the query.\n        </td><td></td></tr><tr><td>spark.databricks.delta.vacuum.logging.enabled</td><td><undefined></td><td>Whether to log vacuum information into the Delta transaction log. 'spark.databricks.delta.commitInfo.enabled' should be enabled when using this config.</td><td></td></tr><tr><td>spark.databricks.delta.vacuum.parallelDelete.enabled</td><td>false</td><td>Enables parallelizing the deletion of files during a vacuum command. Enabling may result hitting rate limits on some storage backends. When enabled, parallelization is controlled by the default number of shuffle partitions.</td><td></td></tr><tr><td>spark.databricks.delta.writeChecksumFile.enabled</td><td>true</td><td>Whether the checksum file can be written.</td><td></td></tr><tr><td>spark.databricks.execution.arrowCollect.maxBytesPerBatch</td><td>131072</td><td>When collecting results as ArrowRecordBatches on executors, limit the maximum number of bytes that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit.</td><td></td></tr><tr><td>spark.databricks.execution.arrowCollect.maxRecordsPerBatch</td><td>1000</td><td>When collecting results as ArrowRecordBatches on executors, limit the maximum number of records that can be written to a single batch in memory. If set to zero or negative there is no limit.</td><td></td></tr><tr><td>spark.databricks.hive.metastore.client.pool.enabled</td><td>true</td><td>When set to true, a pool of hive clients is used</td><td></td></tr><tr><td>spark.databricks.hive.metastore.client.pool.log.fullStackTrace</td><td>true</td><td>Whether or not to log the full stack trace when the object is borrowed</td><td></td></tr><tr><td>spark.databricks.hive.metastore.client.pool.size</td><td>5</td><td>The size of the hive client pool. Only valid when the client pool is enabled</td><td></td></tr><tr><td>spark.databricks.hive.metastore.client.pool.waitTime</td><td>-1</td><td>The max time (ms) a thread waits for a hive client in the pool to become idle</td><td></td></tr><tr><td>spark.databricks.hive.metastore.connection.maxIdleMillis</td><td>60000</td><td>The max time a connection is idle before being closed. Keep this number small to reduce the physical connections, but an aggressive value may increase the connection latency.</td><td></td></tr><tr><td>spark.databricks.io.cache.parquet.enabled</td><td>true</td><td>Enables automatic caching of Parquet files, if spark.databricks.io.cache.enabled is set to true.</td><td></td></tr><tr><td>spark.databricks.io.cache.parquet.numWriters</td><td>1</td><td>The number of asynchronous workers writing Parquet page cache.</td><td></td></tr><tr><td>spark.databricks.io.cache.prefix</td><td>dbfs,s3,hdfs,wasb,adl,abfs,gs,cpfs-</td><td>Only paths with the specified scheme prefixes (separated by comma) are cached. If a path doesn't have a scheme, use the default file system URI to decide.</td><td></td></tr><tr><td>spark.databricks.io.hive.convertMetastoreAvro</td><td>false</td><td>When set to true, the built-in Avro reader and writer are used to process Avro tables created by using the HiveQL syntax, instead of Hive serde. Note Avro logical types are not supported. Please make sure that there are no logical types in Avro Schema if this flag is enabled.</td><td></td></tr><tr><td>spark.databricks.io.parquet.verifyChecksumOnWrite.schemes</td><td>s3,s3a,abfs,abfss,dbfs</td><td>Only paths with the specified schemes (separated by comma) are allowed to enableverification of parquet page checksum on write</td><td></td></tr><tr><td>spark.databricks.optimizer.adaptive.exchange.targetPartitionSize</td><td>67108864b</td><td>The target post-exchange input size in bytes of a task.</td><td></td></tr><tr><td>spark.databricks.optimizer.adaptive.excludedRules</td><td><undefined></td><td>Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have been excluded.</td><td></td></tr><tr><td>spark.databricks.optimizer.dynamicPartitionPruning</td><td>true</td><td>When true, we will generate predicate for partition column when it's used as join key</td><td></td></tr><tr><td>spark.databricks.optimizer.rangeJoin.binSize</td><td>0.0</td><td>Binsize to use with RangeJoin</td><td></td></tr><tr><td>spark.databricks.pyspark.pythonUdfsOnly</td><td>false</td><td>If true, allow only Python UDFs to be executed</td><td></td></tr><tr><td>spark.databricks.service.client.session.cache.size</td><td>20</td><td>The maximum size of the session cache for Spark Service client.</td><td></td></tr><tr><td>spark.databricks.service.dbutils.fs.parallel.ls.threadPoolSize</td><td>20</td><td>Thread pool size for the parallel recursive listing done on the driver for parallel dbutils.fs.[cp/mv/rm] operations.</td><td></td></tr><tr><td>spark.databricks.service.dbutils.fs.parallel.ls.timeoutSeconds</td><td>7200</td><td>Timeout in seconds for the parallel recursive listing done on the driver for parallel dbutils.fs.[cp/mv/rm] operations.</td><td></td></tr><tr><td>spark.databricks.sql.files.prorateMaxPartitionBytes.ubound</td><td>1073741824b</td><td>The upper bound for proration of ConfigEntry(key=spark.sql.files.maxPartitionBytes, defaultValue=128MB, doc=The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC., public=true, version=2.0.0).</td><td></td></tr><tr><td>spark.databricks.sql.minBucketsForBucketedJoin</td><td><undefined></td><td>The minimum number of buckets to perform bucketed join with a bucketed table and a non-bucketed one, to avoid shuffling the bucketed table. If not set, the default value is 'ConfigEntry(key=spark.sql.shuffle.partitions, defaultValue=200, doc=The default number of partitions to use when shuffling data for joins or aggregations. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location., public=true, version=1.1.0)'.</td><td></td></tr><tr><td>spark.databricks.sql.optimizer.maxAliasReplacementProjectListSize</td><td>100</td><td>Alias replacement will be skipped if the number of aliased expressions in a Project, Aggregate or Window plan exceeds this threshold, which can potentially lead to missing some optimization opportunities but will not affect correctness.</td><td></td></tr><tr><td>spark.databricks.streaming.schemaEvolution.enabled</td><td>true</td><td>Whether structured streaming supports schema evolution. If enabled, mirco-batch execution can detect schema change in a batch and triggers schema evolution on the source schema.</td><td></td></tr><tr><td>spark.databricks.thriftserver.getTables.threadPool.size</td><td>64</td><td>The maximum number of threads in the pool used to handle listing of tables of each database within Thriftserver's GetTables operation</td><td></td></tr><tr><td>spark.sql.adaptive.advisoryPartitionSizeInBytes</td><td>67108864b</td><td>The advisory size in bytes of the shuffle partition during adaptive optimization (when 'spark.sql.adaptive.enabled' is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.</td><td>3.0.0</td></tr><tr><td>spark.sql.adaptive.coalescePartitions.enabled</td><td>true</td><td>When true and 'spark.sql.adaptive.enabled' is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks.</td><td>3.0.0</td></tr><tr><td>spark.sql.adaptive.coalescePartitions.initialPartitionNum</td><td><undefined></td><td>The initial number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.</td><td>3.0.0</td></tr><tr><td>spark.sql.adaptive.coalescePartitions.minPartitionNum</td><td><undefined></td><td>The suggested (not guaranteed) minimum number of shuffle partitions after coalescing. If not set, the default value is 2 times the default parallelism of the Spark cluster. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.</td><td>3.0.0</td></tr><tr><td>spark.sql.adaptive.coalescePartitions.minPartitionSize</td><td>1MB</td><td>The minimum size of shuffle partitions after coalescing if 'spark.sql.adaptive.coalescePartitions.minPartitionNum' is not set. When 'spark.sql.adaptive.coalescePartitions.minPartitionNum' is not set, the minimum number of shuffle partitions after coalescing will fall back to 2 times the default parallelism of the Spark cluster, sometimes causing partition sizes to be too small. This config aims to prevent this situation. Note that this config does not take effect if 'spark.sql.adaptive.coalescePartitions.minPartitionNum' is set by the user.</td><td>3.1.0</td></tr><tr><td>spark.sql.adaptive.enabled</td><td>true</td><td>When true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.</td><td>1.6.0</td></tr><tr><td>spark.sql.adaptive.localShuffleReader.enabled</td><td>true</td><td>When true and 'spark.sql.adaptive.enabled' is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.</td><td>3.0.0</td></tr><tr><td>spark.sql.adaptive.optimizer.excludedRules</td><td><undefined></td><td>Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.</td><td>3.1.0</td></tr><tr><td>spark.sql.adaptive.skewJoin.enabled</td><td>true</td><td>When true and 'spark.sql.adaptive.enabled' is true, Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if needed) skewed partitions.</td><td>3.0.0</td></tr><tr><td>spark.sql.adaptive.skewJoin.skewedPartitionFactor</td><td>5</td><td>A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'</td><td>3.0.0</td></tr><tr><td>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</td><td>256MB</td><td>A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Ideally this config should be set larger than 'spark.sql.adaptive.advisoryPartitionSizeInBytes'.</td><td>3.0.0</td></tr><tr><td>spark.sql.ansi.enabled</td><td>false</td><td>When true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results when the inputs to a SQL operator/function are invalid.For full details of this dialect, you can find them in the section \"ANSI Compliance\" of Spark's documentation. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL's style</td><td>3.0.0</td></tr><tr><td>spark.sql.autoBroadcastJoinThreshold</td><td>10MB</td><td>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.  By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command `ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan` has been run, and file-based data source tables where the statistics are computed directly on the files of data.</td><td>1.1.0</td></tr><tr><td>spark.sql.avro.compression.codec</td><td>snappy</td><td>Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.</td><td>2.4.0</td></tr><tr><td>spark.sql.avro.deflate.level</td><td>-1</td><td>Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.</td><td>2.4.0</td></tr><tr><td>spark.sql.avro.filterPushdown.enabled</td><td>true</td><td>When true, enable filter pushdown to Avro datasource.</td><td>3.1.0</td></tr><tr><td>spark.sql.broadcastTimeout</td><td>300</td><td>Timeout in seconds for the broadcast wait time in broadcast joins.</td><td>1.3.0</td></tr><tr><td>spark.sql.bucketing.coalesceBucketsInJoin.enabled</td><td>false</td><td>When true, if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.</td><td>3.1.0</td></tr><tr><td>spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio</td><td>4</td><td>The ratio of the number of two buckets being coalesced should be less than or equal to this value for bucket coalescing to be applied. This configuration only has an effect when 'spark.sql.bucketing.coalesceBucketsInJoin.enabled' is set to true.</td><td>3.1.0</td></tr><tr><td>spark.sql.cache.serializer</td><td>org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer</td><td>The name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.</td><td>3.1.0</td></tr><tr><td>spark.sql.catalog.spark_catalog</td><td><undefined></td><td>A catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog. This catalog shares its identifier namespace with the spark_catalog and must be consistent with it; for example, if a table can be loaded by the spark_catalog, this catalog must also return the table metadata. To delegate operations to the spark_catalog, implementations can extend 'CatalogExtension'.</td><td>3.0.0</td></tr><tr><td>spark.sql.cbo.enabled</td><td>true</td><td>Enables CBO for estimation of plan statistics when set true.</td><td>2.2.0</td></tr><tr><td>spark.sql.cbo.joinReorder.dp.star.filter</td><td>false</td><td>Applies star-join filter heuristics to cost based join enumeration.</td><td>2.2.0</td></tr><tr><td>spark.sql.cbo.joinReorder.dp.threshold</td><td>12</td><td>The maximum number of joined nodes allowed in the dynamic programming algorithm.</td><td>2.2.0</td></tr><tr><td>spark.sql.cbo.joinReorder.enabled</td><td>true</td><td>Enables join reorder in CBO.</td><td>2.2.0</td></tr><tr><td>spark.sql.cbo.planStats.enabled</td><td>false</td><td>When true, the logical plan will fetch row counts and column statistics from catalog.</td><td>3.0.0</td></tr><tr><td>spark.sql.cbo.starSchemaDetection</td><td>false</td><td>When true, it enables join reordering based on star schema detection. </td><td>2.2.0</td></tr><tr><td>spark.sql.columnNameOfCorruptRecord</td><td>_corrupt_record</td><td>The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.</td><td>1.2.0</td></tr><tr><td>spark.sql.csv.filterPushdown.enabled</td><td>true</td><td>When true, enable filter pushdown to CSV datasource.</td><td>3.0.0</td></tr><tr><td>spark.sql.datetime.java8API.enabled</td><td>false</td><td>If the configuration property is set to true, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp and java.sql.Date are used for the same purpose.</td><td>3.0.0</td></tr><tr><td>spark.sql.debug.maxToStringFields</td><td>25</td><td>Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a \"... N more fields\" placeholder.</td><td>3.0.0</td></tr><tr><td>spark.sql.defaultCatalog</td><td>spark_catalog</td><td>Name of the default catalog. This will be the current catalog if users have not explicitly set the current catalog yet.</td><td>3.0.0</td></tr><tr><td>spark.sql.event.truncate.length</td><td>2147483647</td><td>Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.</td><td>3.0.0</td></tr><tr><td>spark.sql.execution.arrow.enabled</td><td>false</td><td>(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.enabled'.)</td><td>2.3.0</td></tr><tr><td>spark.sql.execution.arrow.fallback.enabled</td><td>true</td><td>(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.fallback.enabled'.)</td><td>2.4.0</td></tr><tr><td>spark.sql.execution.arrow.maxRecordsPerBatch</td><td>10000</td><td>When using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit.</td><td>2.3.0</td></tr><tr><td>spark.sql.execution.arrow.pyspark.enabled</td><td>false</td><td>When true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to: 1. pyspark.sql.DataFrame.toPandas 2. pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame The following data types are unsupported: ArrayType of TimestampType, and nested StructType.</td><td>3.0.0</td></tr><tr><td>spark.sql.execution.arrow.pyspark.fallback.enabled</td><td>true</td><td>When true, optimizations enabled by 'spark.sql.execution.arrow.pyspark.enabled' will fallback automatically to non-optimized implementations if an error occurs.</td><td>3.0.0</td></tr><tr><td>spark.sql.execution.arrow.sparkr.enabled</td><td>false</td><td>When true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to: 1. createDataFrame when its input is an R DataFrame 2. collect 3. dapply 4. gapply The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.</td><td>3.0.0</td></tr><tr><td>spark.sql.execution.pandas.udf.buffer.size</td><td>65536</td><td>Same as `spark.buffer.size` but only applies to Pandas UDF executions. If it is not set, the fallback is `spark.buffer.size`. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.</td><td>3.0.0</td></tr><tr><td>spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled</td><td>true</td><td>When true, the traceback from Python UDFs is simplified. It hides the Python worker, (de)serialization, etc from PySpark in tracebacks, and only shows the exception messages from UDFs. Note that this works only with CPython 3.7+.</td><td>3.1.0</td></tr><tr><td>spark.sql.extensions</td><td><undefined></td><td>A comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.</td><td>2.2.0</td></tr><tr><td>spark.sql.files.ignoreCorruptFiles</td><td>false</td><td>Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.</td><td>2.1.1</td></tr><tr><td>spark.sql.files.ignoreMissingFiles</td><td>false</td><td>Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.</td><td>2.3.0</td></tr><tr><td>spark.sql.files.maxPartitionBytes</td><td>128MB</td><td>The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.</td><td>2.0.0</td></tr><tr><td>spark.sql.files.maxRecordsPerFile</td><td>0</td><td>Maximum number of records to write out to a single file. If this value is zero or negative, there is no limit.</td><td>2.2.0</td></tr><tr><td>spark.sql.files.minPartitionNum</td><td><undefined></td><td>The suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is `spark.default.parallelism`. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.</td><td>3.1.0</td></tr><tr><td>spark.sql.function.concatBinaryAsString</td><td>false</td><td>When this option is set to false and all inputs are binary, `functions.concat` returns an output as binary. Otherwise, it returns as a string.</td><td>2.3.0</td></tr><tr><td>spark.sql.function.eltOutputAsString</td><td>false</td><td>When this option is set to false and all inputs are binary, `elt` returns an output as binary. Otherwise, it returns as a string.</td><td>2.3.0</td></tr><tr><td>spark.sql.groupByAliases</td><td>true</td><td>This configuration is only effective when ANSI mode is disabled. When it is true and spark.sql.ansi.enabled is false, aliases in a select list can be used in group by clauses. Otherwise, an analysis exception is thrown in the case.</td><td>2.2.0</td></tr><tr><td>spark.sql.groupByOrdinal</td><td>true</td><td>When true, the ordinal numbers in group by clauses are treated as the position in the select list. When false, the ordinal numbers are ignored.</td><td>2.0.0</td></tr><tr><td>spark.sql.hive.convertInsertingPartitionedTable</td><td>true</td><td>When set to true, and `spark.sql.hive.convertMetastoreParquet` or `spark.sql.hive.convertMetastoreOrc` is true, the built-in ORC/Parquet writer is usedto process inserting into partitioned ORC/Parquet tables created by using the HiveSQL syntax.</td><td>3.0.0</td></tr><tr><td>spark.sql.hive.convertMetastoreCtas</td><td>true</td><td>When set to true,  Spark will try to use built-in data source writer instead of Hive serde in CTAS. This flag is effective only if `spark.sql.hive.convertMetastoreParquet` or `spark.sql.hive.convertMetastoreOrc` is enabled respectively for Parquet and ORC formats</td><td>3.0.0</td></tr><tr><td>spark.sql.hive.convertMetastoreOrc</td><td>true</td><td>When set to true, the built-in ORC reader and writer are used to process ORC tables created by using the HiveQL syntax, instead of Hive serde.</td><td>2.0.0</td></tr><tr><td>spark.sql.hive.convertMetastoreParquet</td><td>true</td><td>When set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde.</td><td>1.1.1</td></tr><tr><td>spark.sql.hive.convertMetastoreParquet.mergeSchema</td><td>false</td><td>When true, also tries to merge possibly different but compatible Parquet schemas in different Parquet data files. This configuration is only effective when \"spark.sql.hive.convertMetastoreParquet\" is true.</td><td>1.3.1</td></tr><tr><td>spark.sql.hive.filesourcePartitionFileCacheSize</td><td>262144000</td><td>When nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.</td><td>2.1.1</td></tr><tr><td>spark.sql.hive.manageFilesourcePartitions</td><td>true</td><td>When true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning.</td><td>2.1.1</td></tr><tr><td>spark.sql.hive.metastore.barrierPrefixes</td><td></td><td>A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).</td><td>1.4.0</td></tr><tr><td>spark.sql.hive.metastore.jars</td><td>/databricks/hive/*</td><td>\n Location of the jars that should be used to instantiate the HiveMetastoreClient.\n This property can be one of four options:\n 1. \"builtin\"\n   Use Hive 2.3.7, which is bundled with the Spark assembly when\n   <code>-Phive</code> is enabled. When this option is chosen,\n   <code>spark.sql.hive.metastore.version</code> must be either\n   <code>2.3.7</code> or not defined.\n 2. \"maven\"\n   Use Hive jars of specified version downloaded from Maven repositories.\n 3. \"path\"\n   Use Hive jars configured by `spark.sql.hive.metastore.jars.path`\n   in comma separated format. Support both local or remote paths.The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n 4. A classpath in the standard format for both Hive and Hadoop. The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n      </td><td>1.4.0</td></tr><tr><td>spark.sql.hive.metastore.jars.path</td><td></td><td>\n Comma-separated paths of the jars that used to instantiate the HiveMetastoreClient.\n This configuration is useful only when `{ConfigEntry(key=spark.sql.hive.metastore.jars, defaultValue=builtin, doc=\n Location of the jars that should be used to instantiate the HiveMetastoreClient.\n This property can be one of four options:\n 1. \"builtin\"\n   Use Hive 2.3.7, which is bundled with the Spark assembly when\n   <code>-Phive</code> is enabled. When this option is chosen,\n   <code>spark.sql.hive.metastore.version</code> must be either\n   <code>2.3.7</code> or not defined.\n 2. \"maven\"\n   Use Hive jars of specified version downloaded from Maven repositories.\n 3. \"path\"\n   Use Hive jars configured by `spark.sql.hive.metastore.jars.path`\n   in comma separated format. Support both local or remote paths.The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n 4. A classpath in the standard format for both Hive and Hadoop. The provided jars\n   should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).\n      , public=true, version=1.4.0).key}` is set as `path`.\n The paths can be any of the following format:\n 1. file://path/to/jar/foo.jar\n 2. hdfs://nameservice/path/to/jar/foo.jar\n 3. /path/to/jar/ (path without URI scheme follow conf `fs.defaultFS`'s URI schema)\n 4. [http/https/ftp]://path/to/jar/foo.jar\n Note that 1, 2, and 3 support wildcard. For example:\n 1. file://path/to/jar/*,file://path2/to/jar/*/*.jar\n 2. hdfs://nameservice/path/to/jar/*,hdfs://nameservice2/path/to/jar/*/*.jar\n      </td><td>3.1.0</td></tr><tr><td>spark.sql.hive.metastore.sharedPrefixes</td><td>org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks</td><td>A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</td><td>1.4.0</td></tr><tr><td>spark.sql.hive.metastore.version</td><td>0.13.0</td><td>Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>.</td><td>1.4.0</td></tr><tr><td>spark.sql.hive.metastorePartitionPruning</td><td>true</td><td>When true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier. This only affects Hive tables not converted to filesource relations (see HiveUtils.CONVERT_METASTORE_PARQUET and HiveUtils.CONVERT_METASTORE_ORC for more information).</td><td>1.5.0</td></tr><tr><td>spark.sql.hive.thriftServer.async</td><td>true</td><td>When set to true, Hive Thrift server executes SQL queries in an asynchronous way.</td><td>1.5.0</td></tr><tr><td>spark.sql.hive.thriftServer.singleSession</td><td>false</td><td>When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.</td><td>1.6.0</td></tr><tr><td>spark.sql.hive.verifyPartitionPath</td><td>false</td><td>When true, check all the partition paths under the table's root directory when reading data stored in HDFS. This configuration will be deprecated in the future releases and replaced by spark.files.ignoreMissingFiles.</td><td>1.4.0</td></tr><tr><td>spark.sql.hive.version</td><td>0.13.0</td><td>deprecated, please use spark.sql.hive.metastore.version to get the Hive version in Spark.</td><td>1.1.1</td></tr><tr><td>spark.sql.inMemoryColumnarStorage.batchSize</td><td>10000</td><td>Controls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</td><td>1.1.1</td></tr><tr><td>spark.sql.inMemoryColumnarStorage.compressed</td><td>true</td><td>When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.</td><td>1.0.1</td></tr><tr><td>spark.sql.inMemoryColumnarStorage.enableVectorizedReader</td><td>true</td><td>Enables vectorized reader for columnar caching.</td><td>2.3.1</td></tr><tr><td>spark.sql.json.filterPushdown.enabled</td><td>true</td><td>When true, enable filter pushdown to JSON datasource.</td><td>3.1.0</td></tr><tr><td>spark.sql.jsonGenerator.ignoreNullFields</td><td>true</td><td>Whether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.</td><td>3.0.0</td></tr><tr><td>spark.sql.legacy.allowHashOnMapType</td><td>false</td><td>When set to true, hash expressions can be applied on elements of MapType. Otherwise, an analysis exception will be thrown.</td><td>3.0.0</td></tr><tr><td>spark.sql.legacy.sessionInitWithConfigDefaults</td><td>false</td><td>Flag to revert to legacy behavior where a cloned SparkSession receives SparkConf defaults, dropping any overrides in its parent SparkSession.</td><td>3.0.0</td></tr><tr><td>spark.sql.mapKeyDedupPolicy</td><td>EXCEPTION</td><td>The policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.</td><td>3.0.0</td></tr><tr><td>spark.sql.maven.additionalRemoteRepositories</td><td>https://maven-central.storage-download.googleapis.com/maven2/</td><td>A comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.</td><td>3.0.0</td></tr><tr><td>spark.sql.maxMetadataStringLength</td><td>100</td><td>Maximum number of characters to output for a metadata string. e.g. file location in `DataSourceScanExec`, every value will be abbreviated if exceed length.</td><td>3.1.0</td></tr><tr><td>spark.sql.maxPlanStringLength</td><td>2147483632</td><td>Maximum number of characters to output for a plan string.  If the plan is longer, further output will be truncated.  The default setting always generates a full plan.  Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.</td><td>3.0.0</td></tr><tr><td>spark.sql.metadataCacheTTLSeconds</td><td>-1000ms</td><td>Time-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (> 0). It also requires setting 'spark.sql.catalogImplementation' to `hive`, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' > 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to `true` to be applied to the partition file metadata cache.</td><td>3.1.0</td></tr><tr><td>spark.sql.optimizer.dynamicPartitionPruning.enabled</td><td>true</td><td>When true, we will generate predicate for partition column when it's used as join key</td><td>3.0.0</td></tr><tr><td>spark.sql.optimizer.enableJsonExpressionOptimization</td><td>true</td><td>Whether to optimize JSON expressions in SQL optimizer. It includes pruning unnecessary columns from from_json, simplifing from_json + to_json, to_json + named_struct(from_json.col1, from_json.col2, ....).</td><td>3.1.0</td></tr><tr><td>spark.sql.optimizer.excludedRules</td><td><undefined></td><td>Configures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded.</td><td>2.4.0</td></tr><tr><td>spark.sql.orc.columnarReaderBatchSize</td><td>4096</td><td>The number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.</td><td>2.4.0</td></tr><tr><td>spark.sql.orc.compression.codec</td><td>snappy</td><td>Sets the compression codec used when writing ORC files. If either `compression` or `orc.compress` is specified in the table-specific options/properties, the precedence would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.Acceptable values include: none, uncompressed, snappy, zlib, lzo.</td><td>2.3.0</td></tr><tr><td>spark.sql.orc.enableVectorizedReader</td><td>true</td><td>Enables vectorized orc decoding.</td><td>2.3.0</td></tr><tr><td>spark.sql.orc.filterPushdown</td><td>true</td><td>When true, enable filter pushdown for ORC files.</td><td>1.4.0</td></tr><tr><td>spark.sql.orc.mergeSchema</td><td>false</td><td>When true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.</td><td>3.0.0</td></tr><tr><td>spark.sql.orderByOrdinal</td><td>true</td><td>When true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.</td><td>2.0.0</td></tr><tr><td>spark.sql.parquet.binaryAsString</td><td>false</td><td>Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</td><td>1.1.1</td></tr><tr><td>spark.sql.parquet.columnarReaderBatchSize</td><td>4096</td><td>The number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.</td><td>2.4.0</td></tr><tr><td>spark.sql.parquet.compression.codec</td><td>snappy</td><td>Sets the compression codec used when writing Parquet files. If either `compression` or `parquet.compression` is specified in the table-specific options/properties, the precedence would be `compression`, `parquet.compression`, `spark.sql.parquet.compression.codec`. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, zstd.</td><td>1.1.1</td></tr><tr><td>spark.sql.parquet.enableVectorizedReader</td><td>true</td><td>Enables vectorized parquet decoding.</td><td>2.0.0</td></tr><tr><td>spark.sql.parquet.filterPushdown</td><td>true</td><td>Enables Parquet filter push-down optimization when set to true.</td><td>1.2.0</td></tr><tr><td>spark.sql.parquet.int96AsTimestamp</td><td>true</td><td>Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</td><td>1.3.0</td></tr><tr><td>spark.sql.parquet.int96TimestampConversion</td><td>false</td><td>This controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive & Spark. Note that this config is ignored when spark.databricks.io.parquet.fastreader.enabled is set to true.</td><td>2.3.0</td></tr><tr><td>spark.sql.parquet.mergeSchema</td><td>false</td><td>When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</td><td>1.5.0</td></tr><tr><td>spark.sql.parquet.outputTimestampType</td><td>INT96</td><td>Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.</td><td>2.3.0</td></tr><tr><td>spark.sql.parquet.recordLevelFilter.enabled</td><td>false</td><td>If true, enables Parquet's native record-level filtering using the pushed down filters. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting 'spark.sql.parquet.enableVectorizedReader' to false.</td><td>2.3.0</td></tr><tr><td>spark.sql.parquet.respectSummaryFiles</td><td>false</td><td>When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.</td><td>1.5.0</td></tr><tr><td>spark.sql.parquet.writeLegacyFormat</td><td>false</td><td>If true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet's fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.</td><td>1.6.0</td></tr><tr><td>spark.sql.parser.quotedRegexColumnNames</td><td>false</td><td>When true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.</td><td>2.3.0</td></tr><tr><td>spark.sql.pivotMaxValues</td><td>10000</td><td>When doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.</td><td>1.6.0</td></tr><tr><td>spark.sql.pyspark.jvmStacktrace.enabled</td><td>false</td><td>When true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled and hides JVM stacktrace and shows a Python-friendly exception only.</td><td>3.0.0</td></tr><tr><td>spark.sql.queryExecutionListeners</td><td><undefined></td><td>List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</td><td>2.3.0</td></tr><tr><td>spark.sql.redaction.options.regex</td><td>(?i)url</td><td>Regex to decide which keys in a Spark SQL command's options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.</td><td>2.2.2</td></tr><tr><td>spark.sql.redaction.string.regex</td><td>(?<![A-Za-z0-9/+=])([A-Za-z0-9/+=]|%2F|%2B|%3D){40}(?=@)</td><td>Regex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from `spark.redaction.string.regex` is used.</td><td>2.3.0</td></tr><tr><td>spark.sql.repl.eagerEval.enabled</td><td>false</td><td>Enables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is supported in PySpark and SparkR. In PySpark, for the notebooks like Jupyter, the HTML table (generated by _repr_html_) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned outputs are showed similar to R data.frame would.</td><td>2.4.0</td></tr><tr><td>spark.sql.repl.eagerEval.maxNumRows</td><td>20</td><td>The max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).</td><td>2.4.0</td></tr><tr><td>spark.sql.repl.eagerEval.truncate</td><td>20</td><td>The max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.</td><td>2.4.0</td></tr><tr><td>spark.sql.session.timeZone</td><td>Etc/UTC</td><td>The ID of session local timezone in the format of either region-based zone IDs or zone offsets. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm' or '(+|-)HH:mm:ss', e.g '-08', '+01:00' or '-13:33:33'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'. Other short names are not recommended to use because they can be ambiguous.</td><td>2.2.0</td></tr><tr><td>spark.sql.shuffle.partitions</td><td>200</td><td>The default number of partitions to use when shuffling data for joins or aggregations. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location.</td><td>1.1.0</td></tr><tr><td>spark.sql.sources.bucketing.autoBucketedScan.enabled</td><td>true</td><td>When true, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan. Note when 'spark.sql.sources.bucketing.enabled' is set to false, this configuration does not take any effect.</td><td>3.1.0</td></tr><tr><td>spark.sql.sources.bucketing.enabled</td><td>true</td><td>When false, we will treat bucketed table as normal table</td><td>2.0.0</td></tr><tr><td>spark.sql.sources.bucketing.maxBuckets</td><td>100000</td><td>The maximum number of buckets allowed.</td><td>2.4.0</td></tr><tr><td>spark.sql.sources.default</td><td>delta</td><td>The default data source to use in input/output.</td><td>1.3.0</td></tr><tr><td>spark.sql.sources.parallelPartitionDiscovery.threshold</td><td>32</td><td>The maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.</td><td>1.5.0</td></tr><tr><td>spark.sql.sources.partitionColumnTypeInference.enabled</td><td>true</td><td>When true, automatically infer the data types for partitioned columns.</td><td>1.5.0</td></tr><tr><td>spark.sql.sources.partitionOverwriteMode</td><td>STATIC</td><td>When INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn't delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(\"partitionOverwriteMode\", \"dynamic\").save(path).</td><td>2.3.0</td></tr><tr><td>spark.sql.statistics.fallBackToHdfs</td><td>false</td><td>When true, it will fall back to HDFS if the table statistics are not available from table metadata. This is useful in determining if a table is small enough to use broadcast joins. This flag is effective only for non-partitioned Hive tables. For non-partitioned data source tables, it will be automatically recalculated if table statistics are not available. For partitioned data source and partitioned Hive tables, It is 'spark.sql.defaultSizeInBytes' if table statistics are not available.</td><td>2.0.0</td></tr><tr><td>spark.sql.statistics.histogram.enabled</td><td>false</td><td>Generates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.</td><td>2.3.0</td></tr><tr><td>spark.sql.statistics.size.autoUpdate.enabled</td><td>false</td><td>Enables automatic update for table size once table's data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.</td><td>2.3.0</td></tr><tr><td>spark.sql.storeAssignmentPolicy</td><td>ANSI</td><td>When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting `string` to `int` or `double` to `boolean`. With legacy policy, Spark allows the type coercion as long as it is a valid `Cast`, which is very loose. e.g. converting `string` to `int` or `double` to `boolean` is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting `double` to `int` or `decimal` to `double` is not allowed.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.checkpointLocation</td><td><undefined></td><td>The default location for storing checkpoint data for streaming queries.</td><td>2.0.0</td></tr><tr><td>spark.sql.streaming.continuous.epochBacklogQueueSize</td><td>10000</td><td>The max number of entries to be stored in queue to wait for late epochs. If this parameter is exceeded by the size of the queue, stream will stop with an error.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.disabledV2Writers</td><td></td><td>A comma-separated list of fully qualified data source register class names for which StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.</td><td>2.3.1</td></tr><tr><td>spark.sql.streaming.fileSource.cleaner.numThreads</td><td>1</td><td>Number of threads used in the file source completed file cleaner.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.forceDeleteTempCheckpointLocation</td><td>false</td><td>When true, enable temporary checkpoint locations force delete.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.metricsEnabled</td><td>false</td><td>Whether Dropwizard/Codahale metrics will be reported for active streaming queries.</td><td>2.0.2</td></tr><tr><td>spark.sql.streaming.multipleWatermarkPolicy</td><td>min</td><td>Policy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is 'min' which chooses the minimum watermark reported across multiple operators. Other alternative value is 'max' which chooses the maximum across multiple operators. Note: This configuration cannot be changed between query restarts from the same checkpoint location.</td><td>2.4.0</td></tr><tr><td>spark.sql.streaming.noDataMicroBatches.enabled</td><td>true</td><td>Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.</td><td>2.4.1</td></tr><tr><td>spark.sql.streaming.numRecentProgressUpdates</td><td>100</td><td>The number of progress updates to retain for a streaming query</td><td>2.1.1</td></tr><tr><td>spark.sql.streaming.stateStore.stateSchemaCheck</td><td>true</td><td>When true, Spark will validate the state schema against schema on existing state and fail query if it's incompatible.</td><td>3.1.0</td></tr><tr><td>spark.sql.streaming.stopActiveRunOnRestart</td><td>true</td><td>Running multiple runs of the same streaming query concurrently is not supported. If we find a concurrent active run for a streaming query (in the same or different SparkSessions on the same cluster) and this flag is true, we will stop the old streaming query run to start the new one.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.stopTimeout</td><td>15s</td><td>How long to wait in milliseconds for the streaming execution thread to stop when calling the streaming query's stop() method. 0 or negative values wait indefinitely.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.streamingQueryListeners</td><td><undefined></td><td>List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</td><td>2.4.0</td></tr><tr><td>spark.sql.streaming.ui.enabled</td><td>true</td><td>Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.ui.retainedProgressUpdates</td><td>100</td><td>The number of progress updates to retain for a streaming query for Structured Streaming UI.</td><td>3.0.0</td></tr><tr><td>spark.sql.streaming.ui.retainedQueries</td><td>100</td><td>The number of inactive queries to retain for Structured Streaming UI.</td><td>3.0.0</td></tr><tr><td>spark.sql.thriftServer.queryTimeout</td><td>0ms</td><td>Set a query duration timeout in seconds in Thrift Server. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. If timeout values are set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller than this configuration value, they take precedence.</td><td>3.1.0</td></tr><tr><td>spark.sql.thriftserver.scheduler.pool</td><td><undefined></td><td>Set a Fair Scheduler pool for a JDBC client session.</td><td>1.1.1</td></tr><tr><td>spark.sql.thriftserver.ui.retainedSessions</td><td>200</td><td>The number of SQL client sessions kept in the JDBC/ODBC web UI history.</td><td>1.4.0</td></tr><tr><td>spark.sql.thriftserver.ui.retainedStatements</td><td>200</td><td>The number of SQL statements kept in the JDBC/ODBC web UI history.</td><td>1.4.0</td></tr><tr><td>spark.sql.ui.explainMode</td><td>formatted</td><td>Configures the query explain mode used in the Spark SQL UI. The value can be 'simple', 'extended', 'codegen', 'cost', or 'formatted'. The default value is 'formatted'.</td><td>3.1.0</td></tr><tr><td>spark.sql.ui.retainedExecutions</td><td>1000</td><td>Number of executions to retain in the Spark UI.</td><td>1.5.0</td></tr><tr><td>spark.sql.variable.substitute</td><td>true</td><td>This enables substitution using syntax like `${var}`, `${system:var}`, and `${env:var}`.</td><td>2.0.0</td></tr><tr><td>spark.sql.warehouse.dir</td><td>/user/hive/warehouse</td><td>The default location for managed databases and tables.</td><td>2.0.0</td></tr><tr><td>spark.thriftserver.arrowBasedRowSet.maxBytesPerFetchLimit</td><td>10485760</td><td>Set a hard limit on the size of the Arrow-based row set constructed per each fetch request, even if the client requests more via TFetchResultsReq.maxBytes.If set to zero or negative there is no limit.</td><td></td></tr><tr><td>spark.thriftserver.arrowBasedRowSet.timestampAsString</td><td>true</td><td>When true, convert timestamp columns to strings</td><td></td></tr><tr><td>spark.thriftserver.cloudStoreBasedRowSet.enabled</td><td>true</td><td>When true, use the CloudStore-based RowSet in Thrift. Otherwise, fall-back to the RowSet resolved by spark.thriftserver.arrowBasedRowSet.enabled.</td><td></td></tr><tr><td>spark.thriftserver.cloudStoreBasedRowSet.executor.cloudUploadThreshold</td><td>1048576b</td><td>If the result size is below this threshold, arrow batches will returned instead  of cloud files</td><td></td></tr><tr><td>spark.thriftserver.cloudStoreBasedRowSet.maxBytesPerFetchLimit</td><td>1073741824</td><td>Set a hard limit on the size of the cloud-based row set constructed per each fetch request. If set to zero or negative there is no limit.</td><td></td></tr><tr><td>spark.thriftserver.cloudStoreBasedRowSet.smallResultsOptimization.enabled</td><td>true</td><td>When true, smaller results sets will be returned as arrow batches instead of cloud files.</td><td></td></tr><tr><td>spark.thriftserver.cloudfetch.enabled</td><td>true</td><td>When true, use the CloudStore-based RowSet in Thrift. Otherwise, fall-back to the RowSet resolved by spark.thriftserver.arrowBasedRowSet.enabled.</td><td></td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["spark.databricks.preemption.enabled","true"],["spark.driver.tempDirectory","/local_disk0/tmp"],["spark.sql.streaming.checkpointFileManagerClass","com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager"],["spark.databricks.service.dbutils.repl.backend","com.databricks.dbconnect.ReplDBUtils"],["spark.streaming.driver.writeAheadLog.allowBatching","true"],["spark.databricks.clusterSource","UI"],["spark.hadoop.hive.server2.transport.mode","http"],["spark.app.id","local-1634401758399"],["spark.executor.memory","8278m"],["spark.hadoop.fs.cpfs-adl.impl.disable.cache","true"],["spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException","true"],["spark.databricks.clusterUsageTags.hailEnabled","false"],["spark.hadoop.fs.mcfs-s3.impl","com.databricks.sql.acl.fs.ManagedCatalogFileSystem"],["spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled","false"],["spark.databricks.clusterUsageTags.containerType","LXC"],["spark.eventLog.enabled","false"],["spark.databricks.clusterUsageTags.isIMv2Enabled","false"],["spark.databricks.clusterUsageTags.containerZoneId","us-west-2a"],["spark.executor.tempDirectory","/local_disk0/tmp"],["spark.databricks.sparkContextId","5838928010713590200"],["spark.hadoop.mapred.output.committer.class","com.databricks.backend.daemon.data.client.DirectOutputCommitter"],["spark.hadoop.hive.server2.thrift.http.port","10000"],["spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version","2"],["spark.sql.allowMultipleContexts","false"],["spark.home","/databricks/spark"],["spark.databricks.clusterUsageTags.clusterTargetWorkers","0"],["spark.hadoop.hive.server2.idle.operation.timeout","7200000"],["spark.task.reaper.enabled","true"],["spark.storage.memoryFraction","0.5"],["spark.databricks.clusterUsageTags.clusterFirstOnDemand","0"],["spark.driver.maxResultSize","4g"],["spark.databricks.delta.multiClusterWrites.enabled","true"],["spark.worker.cleanup.enabled","false"],["spark.sql.legacy.createHiveTableByDefault","false"],["spark.databricks.workspace.matplotlibInline.enabled","true"],["spark.databricks.clusterUsageTags.enableCredentialPassthrough","false"],["spark.databricks.clusterUsageTags.driverPublicDns","ec2-52-33-44-126.us-west-2.compute.amazonaws.com"],["spark.databricks.clusterUsageTags.enableJdbcAutoStart","true"],["spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType","ebs_volume_type: GENERAL_PURPOSE_SSD\n"],["spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough","false"],["spark.ui.port","46359"],["spark.hadoop.fs.wasb.impl.disable.cache","true"],["spark.databricks.clusterUsageTags.clusterLogDestination",""],["spark.cleaner.referenceTracking.blocking","false"],["spark.executor.extraClassPath","/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.joshelser--dropwizard-metrics-hadoop-metrics2-reporter--com.github.joshelser__dropwizard-metrics-hadoop-metrics2-reporter__0.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-common--org.apache.hive__hive-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-exec-core--org.apache.hive__hive-exec-core__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-vector-code-gen--org.apache.hive__hive-vector-code-gen__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.mesos--mesos-shaded-protobuf--org.apache.mesos__mesos-shaded-protobuf__1.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-core--org.apache.orc__orc-core__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-mapreduce--org.apache.orc__orc-mapreduce__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.xbean--xbean-asm7-shaded--org.apache.xbean__xbean-asm7-shaded__4.15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util-ajax--org.eclipse.jetty__jetty-util-ajax__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--jakarta.inject--org.glassfish.hk2.external__jakarta.inject__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.inject--jersey-hk2--org.glassfish.jersey.inject__jersey-hk2__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.hibernate.validator--hibernate-validator--org.hibernate.validator__hibernate-validator__6.1.0.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.javassist--javassist--org.javassist__javassist__3.25.0-GA.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.3.2.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-ast_2.12--org.json4s__json4s-ast_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-core_2.12--org.json4s__json4s-core_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-jackson_2.12--org.json4s__json4s-jackson_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-scalap_2.12--org.json4s__json4s-scalap_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.lz4--lz4-java--org.lz4__lz4-java__1.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.2.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--shims--org.roaringbitmap__shims__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__6.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-compiler_2.12--org.scala-lang__scala-compiler__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-library_2.12--org.scala-lang__scala-library__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-reflect_2.12--org.scala-lang__scala-reflect__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-collection-compat_2.12--org.scala-lang.modules__scala-collection-compat_2.12__2.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-parser-combinators_2.12--org.scala-lang.modules__scala-parser-combinators_2.12__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-xml_2.12--org.scala-lang.modules__scala-xml_2.12__1.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalacheck--scalacheck_2.12--org.scalacheck__scalacheck_2.12__1.14.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalactic--scalactic_2.12--org.scalactic__scalactic_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze-macros_2.12--org.scalanlp__breeze-macros_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze_2.12--org.scalanlp__breeze_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalatest--scalatest_2.12--org.scalatest__scalatest_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.threeten--threeten-extra--org.threeten__threeten-extra__1.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.tukaani--xz--org.tukaani__xz__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--algebra_2.12--org.typelevel__algebra_2.12__2.0.0-M2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--cats-kernel_2.12--org.typelevel__cats-kernel_2.12__2.0.0-M4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--machinist_2.12--org.typelevel__machinist_2.12__0.6.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--macro-compat_2.12--org.typelevel__macro-compat_2.12__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-macros_2.12--org.typelevel__spire-macros_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-platform_2.12--org.typelevel__spire-platform_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-util_2.12--org.typelevel__spire-util_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire_2.12--org.typelevel__spire_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.wildfly.openssl--wildfly-openssl--org.wildfly.openssl__wildfly-openssl__1.0.7.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.yaml--snakeyaml--org.yaml__snakeyaml__1.24.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--pl.edu.icm--JLargeArrays--pl.edu.icm__JLargeArrays__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--stax--stax-api--stax__stax-api__1.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.yetus--audience-annotations--org.apache.yetus__audience-annotations__0.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.14.jar:/databricks/jars/----workspace_spark_3_1--mllib--libmllib_resources.jar:/databricks/jars/----workspace_spark_3_1--mllib--mllib_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--mllib--org.jpmml__pmml-model__1.4.8_shaded-for-mllib.jar:/databricks/jars/----workspace_spark_3_1--mllib-local--mllib-local-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--py4j--py4j-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--repl--repl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--catalyst-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libcatalyst_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libspark-sql-parser-compiled.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--core--proto_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--spark-sql-databricks-command-parser_java_out.srcjar:/databricks/jars/----workspace_spark_3_1--sql--hive--hive_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--libhive_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--org.apache.commons__commons-pool2__2.6.2_shaded-for-hive.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-protocol-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--streaming--libstreaming_resources.jar:/databricks/jars/----workspace_spark_3_1--streaming--streaming-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--fatJar-assembly-0.18.1-SNAPSHOT.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--gcs-connector-hadoop2-2.0.0-shaded.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core-http-netty__1.6.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core__1.8.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-identity__1.1.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-security-keyvault-keys__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.dataformat__jackson-dataformat-xml__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.datatype__jackson-datatype-jsr310__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.module__jackson-module-jaxb-annotations__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.woodstox__woodstox-core__6.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml__aalto-xml__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.github.stephenc.jcip__jcip-annotations__1.0-1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.madgag.spongycastle__core__1.54.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j-persistence-extension__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j__1.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.sqlserver__mssql-jdbc__9.2.1.jre8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__content-type__2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__lang-tag__1.4.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__nimbus-jose-jwt__8.8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__oauth2-oidc-sdk__7.1.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-buffer__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-resolver__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-kqueue-osx-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor.netty__reactor-netty__0.9.11.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor__reactor-core__3.3.9.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.activation__jakarta.activation-api__1.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.xml.bind__jakarta.xml.bind-api__2.3.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.java.dev.jna__jna-platform__5.6.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__accessors-smart__1.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__json-smart__2.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.bouncycastle__bcprov-jdk15on__1.69_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.codehaus.woodstox__stax2-api__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.jetbrains__annotations__15.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-dom__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-jaxb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdbx__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-simple__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__database__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.nanohttpd__nanohttpd__2.3.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.ow2.asm__asm__5.0.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.simpleframework__simple-xml__2.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--stax__stax__1.2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--xpp3__xpp3__1.1.3.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--avro_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--libavro_resources_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--file-notification-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--libfile-notification-common_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider--libkafka-0-10-token-provider_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider-unshaded_2.12_deploy_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--libkafka-0-10-resources_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--org.apache.kafka__kafka-clients__2.6.0_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--redshift--redshift-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--libmetrics-ganglia.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--spark-ganglia-lgpl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--libsql-aws-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--sql-aws-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--libsql-azure-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--sql-azure-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-dw--sql-dw-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/acl--api--helpers--helpers-spark_3.1_2.12_deploy.jar:/databricks/jars/acl--auth--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/api--common--workspace-spark_3.1_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base_java-spark_3.1_2.12_deploy.jar:/databricks/jars/central--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.1_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--cloudstorage--presigned-url-spark_3.1_2.12_deploy.jar:/databricks/jars/common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.1_2.12_deploy.jar:/databricks/jars/common--crypto-providers--amazon-corretto-crypto-provider--libamazon-corretto-crypto-provider.jar:/databricks/jars/common--hadoop--hadoop-spark_3.1_2.12_deploy.jar:/databricks/jars/common--java-flight-recorder--java-flight-recorder-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jupyter-utils--jupyter_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.1_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--network--network-spark_3.1_2.12_deploy.jar:/databricks/jars/common--node-types--node-types-spark_3.1_2.12_deploy.jar:/databricks/jars/common--path--path-spark_3.1_2.12_deploy.jar:/databricks/jars/common--pricing--pricing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.1_2.12_deploy.jar:/databricks/jars/common--reflection--reflection-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage-driver-utils--storage_driver_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--util--locks-spark_3.1_2.12_deploy.jar:/databricks/jars/credentials-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/jsonutil--jsonutil-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--ratelimitedlogger--ratelimitedlogger-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--sourcecode--sourcecode-spark_3.1_2.12_deploy.jar:/databricks/jars/managed-catalog--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/maven-trees--amazon-corretto-crypto-provider--software.amazon.cryptools--AmazonCorrettoCryptoProvider-linux-x86_64--software.amazon.cryptools__AmazonCorrettoCryptoProvider-linux-x86_64__1.4.0.jar:/databricks/jars/s3--s3-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/secret-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--command-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.1_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__2.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.4.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__30.0-android_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.14.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.6.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.23.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_javax.annotation_javax.annotation-api_javax.annotation__javax.annotation-api__1.3.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.2.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/third_party--armeria--service_discovery-resources.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--annotations_shaded.jar:/databricks/jars/third_party--gcs-private--api-common_shaded.jar:/databricks/jars/third_party--gcs-private--auto-value-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--checker-compat-qual_shaded.jar:/databricks/jars/third_party--gcs-private--checker-qual_shaded.jar:/databricks/jars/third_party--gcs-private--commons-codec_shaded.jar:/databricks/jars/third_party--gcs-private--commons-lang3_shaded.jar:/databricks/jars/third_party--gcs-private--commons-logging_shaded.jar:/databricks/jars/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--gcs-private--error_prone_annotations_shaded.jar:/databricks/jars/third_party--gcs-private--failureaccess_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-system-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-connector_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-shaded-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcsio_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-storage_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--gcs-private--google-extensions_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-alts_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-api_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-auth_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-context_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-core_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-grpclb_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-stub_shaded.jar:/databricks/jars/third_party--gcs-private--gson_shaded.jar:/databricks/jars/third_party--gcs-private--guava_shaded.jar:/databricks/jars/third_party--gcs-private--httpclient_shaded.jar:/databricks/jars/third_party--gcs-private--httpcore_shaded.jar:/databricks/jars/third_party--gcs-private--j2objc-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--jackson-core_shaded.jar:/databricks/jars/third_party--gcs-private--javax.annotation-api_shaded.jar:/databricks/jars/third_party--gcs-private--jsr305_shaded.jar:/databricks/jars/third_party--gcs-private--listenablefuture_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-api_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--gcs-private--perfmark-api_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java-util_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java_shaded.jar:/databricks/jars/third_party--gcs-private--util-hadoop_shaded.jar:/databricks/jars/third_party--gcs-private--util_shaded.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.1_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-annotations_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-core_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-databind_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-api_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-impl_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-jackson_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--zeromq--jeromq_shaded.jar:/databricks/jars/third_party--zeromq--jnacl_shaded.jar:/databricks/jars/utils--process_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.1_2.12_deploy.jar"],["spark.databricks.clusterUsageTags.isSingleUserCluster","false"],["spark.databricks.clusterUsageTags.clusterState","Pending"],["spark.databricks.tahoe.logStore.azure.class","com.databricks.tahoe.store.AzureLogStore"],["spark.hadoop.fs.azure.skip.metrics","true"],["spark.hadoop.fs.s3.impl","shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem"],["spark.scheduler.mode","FAIR"],["spark.sql.sources.default","delta"],["spark.hadoop.fs.cpfs-s3n.impl","com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"],["spark.databricks.clusterUsageTags.clusterWorkers","0"],["spark.hadoop.fs.cpfs-adl.impl","com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"],["spark.hadoop.fs.cpfs-abfss.impl","com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"],["spark.databricks.passthrough.oauth.refresher.impl","com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient"],["spark.sql.hive.metastore.sharedPrefixes","org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks"],["spark.databricks.io.directoryCommit.enableLogicalDelete","false"],["spark.task.reaper.killTimeout","60s"],["spark.hadoop.parquet.block.size.row.check.min","10"],["spark.hadoop.hive.server2.use.SSL","true"],["spark.hadoop.fs.mcfs-s3a.impl","com.databricks.sql.acl.fs.ManagedCatalogFileSystem"],["spark.databricks.clusterUsageTags.clusterAvailability","ON_DEMAND"],["spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb","0"],["spark.hadoop.hive.server2.keystore.path","/databricks/keys/jetty-ssl-driver-keystore.jks"],["spark.databricks.credential.redactor","com.databricks.logging.secrets.CredentialRedactorProxyImpl"],["spark.databricks.clusterUsageTags.clusterPinned","false"],["spark.databricks.acl.provider","com.databricks.sql.acl.ReflectionBackedAclProvider"],["spark.extraListeners","com.databricks.backend.daemon.driver.DBCEventLoggingListener"],["spark.sql.parquet.cacheMetadata","true"],["spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2","0"],["spark.hadoop.fs.adl.impl","com.databricks.adl.AdlFileSystem"],["spark.hadoop.fs.cpfs-abfss.impl.disable.cache","true"],["spark.databricks.clusterUsageTags.enableLocalDiskEncryption","false"],["spark.databricks.tahoe.logStore.class","com.databricks.tahoe.store.DelegatingLogStore"],["libraryDownload.sleepIntervalSeconds","5"],["spark.databricks.cloudProvider","AWS"],["spark.sql.hive.convertMetastoreParquet","true"],["spark.executor.id","driver"],["spark.databricks.service.dbutils.server.backend","com.databricks.dbconnect.SparkServerDBUtils"],["spark.databricks.clusterUsageTags.workerEnvironmentId","default-worker-env"],["spark.databricks.repl.enableClassFileCleanup","true"],["spark.sql.catalogImplementation","hive"],["spark.databricks.managedCatalog.s3a.tokenProviderClassName","com.databricks.backend.daemon.driver.aws.ManagedCatalogS3TokenProvider"],["spark.hadoop.fs.s3a.multipart.size","10485760"],["spark.databricks.clusterUsageTags.cloudProvider","AWS"],["spark.metrics.conf","/databricks/spark/conf/metrics.properties"],["spark.akka.frameSize","256"],["spark.hadoop.fs.s3a.fast.upload","true"],["spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled","true"],["spark.sql.streaming.stopTimeout","15s"],["spark.hadoop.hive.server2.keystore.password","[REDACTED]"],["spark.hadoop.fs.wasbs.impl","shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem"],["spark.databricks.overrideDefaultCommitProtocol","org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol"],["spark.databricks.clusterUsageTags.clusterNoDriverDaemon","false"],["libraryDownload.timeoutSeconds","180"],["spark.hadoop.parquet.memory.pool.ratio","0.5"],["spark.databricks.clusterUsageTags.clusterScalingType","fixed_size"],["spark.sql.hive.metastore.jars","/databricks/hive/*"],["spark.databricks.passthrough.adls.gen2.tokenProviderClassName","com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider"],["spark.databricks.tahoe.logStore.gcp.class","com.databricks.tahoe.store.GCPLogStore"],["spark.serializer.objectStreamReset","100"],["spark.sql.sources.commitProtocolClass","com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol"],["spark.hadoop.fs.abfss.impl","shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem"],["spark.databricks.clusterUsageTags.clusterPythonVersion","3"],["spark.databricks.clusterUsageTags.enableDfAcls","false"],["spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount","0"],["spark.shuffle.service.enabled","true"],["spark.hadoop.fs.file.impl","com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem"],["spark.hadoop.fs.mcfs-s3n.impl","com.databricks.sql.acl.fs.ManagedCatalogFileSystem"],["spark.hadoop.fs.cpfs-s3.impl","com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"],["spark.hadoop.fs.s3a.multipart.threshold","104857600"],["spark.rpc.message.maxSize","256"],["spark.databricks.driverNfs.enabled","true"],["spark.databricks.clusterUsageTags.clusterMetastoreAccessType","RDS_DIRECT"],["spark.databricks.clusterUsageTags.ngrokNpipEnabled","false"],["spark.databricks.clusterUsageTags.instanceProfileUsed","false"],["spark.r.sql.derby.temp.dir","/tmp/RtmpT4QJ8k"],["spark.hadoop.fs.abfs.impl","shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem"],["spark.app.startTime","1634401753697"],["spark.databricks.acl.scim.client","com.databricks.spark.sql.acl.client.DriverToWebappScimClient"],["spark.databricks.clusterUsageTags.driverInstancePrivateIp","10.172.179.45"],["spark.hadoop.fs.adl.impl.disable.cache","true"],["spark.hadoop.parquet.block.size.row.check.max","10"],["spark.hadoop.fs.s3a.connection.maximum","200"],["spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2","0"],["spark.executor.extraJavaOptions","-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1"],["spark.hadoop.fs.s3a.fast.upload.active.blocks","32"],["spark.shuffle.reduceLocality.enabled","false"],["spark.databricks.clusterUsageTags.driverNodeType","dev-tier-node"],["spark.hadoop.spark.sql.sources.outputCommitterClass","com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter"],["spark.hadoop.fs.AbstractFileSystem.gs.impl","shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"],["spark.databricks.clusterUsageTags.instanceBootstrapType","ssh"],["spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled","false"],["spark.databricks.driverNodeTypeId","dev-tier-node"],["spark.sql.parquet.compression.codec","snappy"],["spark.databricks.cloudfetch.hasRegionSupport","true"],["spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories","false"],["spark.hadoop.fs.wasb.impl","shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem"],["spark.hadoop.fs.mcfs-abfss.impl.disable.cache","true"],["spark.databricks.workerNodeTypeId","dev-tier-node"],["spark.databricks.clusterUsageTags.clusterEbsVolumeSize","0"],["spark.sql.warehouse.dir","/user/hive/warehouse"],["spark.databricks.passthrough.s3a.tokenProviderClassName","com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider"],["spark.databricks.session.share","false"],["spark.databricks.clusterUsageTags.driverContainerId","83d618269aee420ca7c77aea0e1025e6"],["spark.databricks.clusterUsageTags.clusterResourceClass","default"],["spark.driver.port","39685"],["spark.databricks.clusterUsageTags.clusterSku","STANDARD_SKU"],["spark.hadoop.fs.gs.impl.disable.cache","true"],["spark.databricks.clusterUsageTags.clusterEbsVolumeType","GENERAL_PURPOSE_SSD"],["spark.hadoop.parquet.page.size.check.estimate","false"],["spark.hadoop.spark.driverproxy.customHeadersToProperties","X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name"],["spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class","com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory"],["spark.databricks.delta.preview.enabled","true"],["spark.databricks.cloudfetch.requesterClassName","com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester"],["spark.master","local[8]"],["spark.databricks.delta.logStore.crossCloud.fatal","true"],["spark.files.fetchFailure.unRegisterOutputOnHost","true"],["spark.databricks.clusterUsageTags.sparkVersion","8.3.x-scala2.12"],["spark.databricks.clusterUsageTags.enableSqlAclsOnly","false"],["spark.databricks.clusterUsageTags.clusterEbsVolumeCount","0"],["spark.databricks.clusterUsageTags.clusterNumSshKeys","0"],["spark.hadoop.fs.gs.outputstream.upload.chunk.size","16777216"],["spark.databricks.tahoe.logStore.aws.class","com.databricks.tahoe.store.S3LockBasedLogStore"],["spark.speculation.quantile","0.9"],["spark.databricks.clusterUsageTags.privateLinkEnabled","false"],["spark.shuffle.manager","SORT"],["spark.files.overwrite","true"],["spark.r.numRBackendThreads","1"],["spark.hadoop.fs.wasbs.impl.disable.cache","true"],["spark.hadoop.fs.abfss.impl.disable.cache","true"],["spark.databricks.clusterUsageTags.clusterOwnerUserId","4661816221656833"],["spark.databricks.workspace.multipleResults.enabled","true"],["spark.sql.hive.metastore.version","0.13.0"],["spark.shuffle.service.port","4048"],["spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType","default"],["spark.databricks.acl.client","com.databricks.spark.sql.acl.client.SparkSqlAclClient"],["spark.streaming.driver.writeAheadLog.closeFileAfterWrite","true"],["spark.hadoop.hive.warehouse.subdir.inherit.perms","false"],["spark.hadoop.fs.mcfs-abfss.impl","com.databricks.sql.acl.fs.ManagedCatalogFileSystem"],["spark.hadoop.fs.s3n.impl","shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem"],["spark.databricks.clusterUsageTags.enableElasticDisk","false"],["spark.databricks.clusterUsageTags.clusterId","1016-162341-65m0wgy"],["spark.databricks.clusterUsageTags.clusterNodeType","dev-tier-node"],["spark.databricks.passthrough.adls.tokenProviderClassName","com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider"],["spark.app.name","Databricks Shell"],["spark.driver.allowMultipleContexts","false"],["spark.rdd.compress","true"],["spark.databricks.python.defaultPythonRepl","ipykernel"],["spark.databricks.eventLog.dir","eventlogs"],["spark.databricks.driverNfs.pathSuffix",".ephemeral_nfs"],["spark.databricks.clusterUsageTags.clusterCreator","Webapp"],["spark.speculation","false"],["spark.hadoop.databricks.dbfs.client.version","v1"],["spark.hadoop.hive.server2.session.check.interval","60000"],["spark.sql.hive.convertCTAS","true"],["spark.hadoop.spark.sql.parquet.output.committer.class","org.apache.spark.sql.parquet.DirectParquetOutputCommitter"],["spark.hadoop.fs.gs.impl","shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"],["spark.hadoop.fs.s3a.fast.upload.default","true"],["spark.databricks.clusterUsageTags.clusterGeneration","0"],["spark.hadoop.fs.abfs.impl.disable.cache","true"],["spark.speculation.multiplier","3"],["spark.databricks.clusterUsageTags.clusterOwnerOrgId","2811288910448988"],["spark.storage.blockManagerTimeoutIntervalMs","300000"],["spark.driver.host","10.172.162.142"],["spark.databricks.clusterUsageTags.instanceWorkerEnvId","default-worker-env"],["spark.repl.class.outputDir","/local_disk0/tmp/repl/spark-5838928010713590200-66bd2a8f-f53a-4a43-ad10-4732052981f2"],["spark.databricks.clusterUsageTags.driverContainerPrivateIp","10.172.162.142"],["spark.sparkr.use.daemon","false"],["spark.scheduler.listenerbus.eventqueue.capacity","20000"],["spark.databricks.clusterUsageTags.clusterStateMessage","Starting Spark"],["spark.hadoop.parquet.page.write-checksum.enabled","true"],["spark.hadoop.databricks.s3commit.client.sslTrustAll","false"],["spark.hadoop.fs.s3a.threads.max","136"],["spark.repl.class.uri","spark://10.172.162.142:39685/classes"],["spark.r.backendConnectionTimeout","604800"],["spark.hadoop.hive.server2.idle.session.timeout","900000"],["spark.databricks.redactor","com.databricks.spark.util.DatabricksSparkLogRedactorProxy"],["spark.databricks.clusterUsageTags.autoTerminationMinutes","120"],["spark.hadoop.fs.s3a.impl","shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem"],["spark.hadoop.parquet.page.verify-checksum.enabled","true"],["spark.databricks.clusterUsageTags.dataPlaneRegion","us-west-2"],["spark.logConf","true"],["spark.databricks.clusterUsageTags.enableJobsAutostart","true"],["spark.hadoop.hive.server2.enable.doAs","false"],["eventLog.rolloverIntervalSeconds","3600"],["spark.shuffle.memoryFraction","0.2"],["spark.databricks.clusterUsageTags.clusterAllTags","[{\"key\":\"Name\",\"value\":\"ce-worker\"}]"],["spark.databricks.clusterUsageTags.driverInstanceId","i-0a4c6c24cf2ad3d92"],["spark.databricks.clusterUsageTags.clusterName","manning_project"],["spark.hadoop.fs.cpfs-s3a.impl","com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"],["spark.databricks.clusterUsageTags.region","us-west-2"],["spark.databricks.clusterUsageTags.clusterSpotBidPricePercent","100"],["spark.files.useFetchCache","false"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_1","type":"\"string\"","metadata":"{}"},{"name":"_2","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th></tr></thead><tbody><tr><td>spark.databricks.preemption.enabled</td><td>true</td></tr><tr><td>spark.driver.tempDirectory</td><td>/local_disk0/tmp</td></tr><tr><td>spark.sql.streaming.checkpointFileManagerClass</td><td>com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager</td></tr><tr><td>spark.databricks.service.dbutils.repl.backend</td><td>com.databricks.dbconnect.ReplDBUtils</td></tr><tr><td>spark.streaming.driver.writeAheadLog.allowBatching</td><td>true</td></tr><tr><td>spark.databricks.clusterSource</td><td>UI</td></tr><tr><td>spark.hadoop.hive.server2.transport.mode</td><td>http</td></tr><tr><td>spark.app.id</td><td>local-1634401758399</td></tr><tr><td>spark.executor.memory</td><td>8278m</td></tr><tr><td>spark.hadoop.fs.cpfs-adl.impl.disable.cache</td><td>true</td></tr><tr><td>spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.hailEnabled</td><td>false</td></tr><tr><td>spark.hadoop.fs.mcfs-s3.impl</td><td>com.databricks.sql.acl.fs.ManagedCatalogFileSystem</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.containerType</td><td>LXC</td></tr><tr><td>spark.eventLog.enabled</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.isIMv2Enabled</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.containerZoneId</td><td>us-west-2a</td></tr><tr><td>spark.executor.tempDirectory</td><td>/local_disk0/tmp</td></tr><tr><td>spark.databricks.sparkContextId</td><td>5838928010713590200</td></tr><tr><td>spark.hadoop.mapred.output.committer.class</td><td>com.databricks.backend.daemon.data.client.DirectOutputCommitter</td></tr><tr><td>spark.hadoop.hive.server2.thrift.http.port</td><td>10000</td></tr><tr><td>spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version</td><td>2</td></tr><tr><td>spark.sql.allowMultipleContexts</td><td>false</td></tr><tr><td>spark.home</td><td>/databricks/spark</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterTargetWorkers</td><td>0</td></tr><tr><td>spark.hadoop.hive.server2.idle.operation.timeout</td><td>7200000</td></tr><tr><td>spark.task.reaper.enabled</td><td>true</td></tr><tr><td>spark.storage.memoryFraction</td><td>0.5</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterFirstOnDemand</td><td>0</td></tr><tr><td>spark.driver.maxResultSize</td><td>4g</td></tr><tr><td>spark.databricks.delta.multiClusterWrites.enabled</td><td>true</td></tr><tr><td>spark.worker.cleanup.enabled</td><td>false</td></tr><tr><td>spark.sql.legacy.createHiveTableByDefault</td><td>false</td></tr><tr><td>spark.databricks.workspace.matplotlibInline.enabled</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.enableCredentialPassthrough</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.driverPublicDns</td><td>ec2-52-33-44-126.us-west-2.compute.amazonaws.com</td></tr><tr><td>spark.databricks.clusterUsageTags.enableJdbcAutoStart</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType</td><td>ebs_volume_type: GENERAL_PURPOSE_SSD\n</td></tr><tr><td>spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough</td><td>false</td></tr><tr><td>spark.ui.port</td><td>46359</td></tr><tr><td>spark.hadoop.fs.wasb.impl.disable.cache</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterLogDestination</td><td></td></tr><tr><td>spark.cleaner.referenceTracking.blocking</td><td>false</td></tr><tr><td>spark.executor.extraClassPath</td><td>/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.joshelser--dropwizard-metrics-hadoop-metrics2-reporter--com.github.joshelser__dropwizard-metrics-hadoop-metrics2-reporter__0.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-common--org.apache.hive__hive-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-exec-core--org.apache.hive__hive-exec-core__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-vector-code-gen--org.apache.hive__hive-vector-code-gen__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.mesos--mesos-shaded-protobuf--org.apache.mesos__mesos-shaded-protobuf__1.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-core--org.apache.orc__orc-core__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-mapreduce--org.apache.orc__orc-mapreduce__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1-databricks9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.xbean--xbean-asm7-shaded--org.apache.xbean__xbean-asm7-shaded__4.15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.16.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util-ajax--org.eclipse.jetty__jetty-util-ajax__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.4.36.v20210114.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.hk2.external--jakarta.inject--org.glassfish.hk2.external__jakarta.inject__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.inject--jersey-hk2--org.glassfish.jersey.inject__jersey-hk2__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.hibernate.validator--hibernate-validator--org.hibernate.validator__hibernate-validator__6.1.0.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.javassist--javassist--org.javassist__javassist__3.25.0-GA.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.3.2.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-ast_2.12--org.json4s__json4s-ast_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-core_2.12--org.json4s__json4s-core_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-jackson_2.12--org.json4s__json4s-jackson_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.json4s--json4s-scalap_2.12--org.json4s__json4s-scalap_2.12__3.7.0-M5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.lz4--lz4-java--org.lz4__lz4-java__1.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.2.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.roaringbitmap--shims--org.roaringbitmap__shims__0.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__6.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-compiler_2.12--org.scala-lang__scala-compiler__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-library_2.12--org.scala-lang__scala-library__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-reflect_2.12--org.scala-lang__scala-reflect__2.12.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-collection-compat_2.12--org.scala-lang.modules__scala-collection-compat_2.12__2.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-parser-combinators_2.12--org.scala-lang.modules__scala-parser-combinators_2.12__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-xml_2.12--org.scala-lang.modules__scala-xml_2.12__1.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalacheck--scalacheck_2.12--org.scalacheck__scalacheck_2.12__1.14.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalactic--scalactic_2.12--org.scalactic__scalactic_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze-macros_2.12--org.scalanlp__breeze-macros_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalanlp--breeze_2.12--org.scalanlp__breeze_2.12__1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalatest--scalatest_2.12--org.scalatest__scalatest_2.12__3.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.threeten--threeten-extra--org.threeten__threeten-extra__1.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.tukaani--xz--org.tukaani__xz__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--algebra_2.12--org.typelevel__algebra_2.12__2.0.0-M2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--cats-kernel_2.12--org.typelevel__cats-kernel_2.12__2.0.0-M4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--machinist_2.12--org.typelevel__machinist_2.12__0.6.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--macro-compat_2.12--org.typelevel__macro-compat_2.12__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-macros_2.12--org.typelevel__spire-macros_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-platform_2.12--org.typelevel__spire-platform_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire-util_2.12--org.typelevel__spire-util_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.typelevel--spire_2.12--org.typelevel__spire_2.12__0.17.0-M1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.wildfly.openssl--wildfly-openssl--org.wildfly.openssl__wildfly-openssl__1.0.7.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.yaml--snakeyaml--org.yaml__snakeyaml__1.24.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--pl.edu.icm--JLargeArrays--pl.edu.icm__JLargeArrays__1.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--stax--stax-api--stax__stax-api__1.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.yetus--audience-annotations--org.apache.yetus__audience-annotations__0.5.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--zookeeper-3.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.14.jar:/databricks/jars/----workspace_spark_3_1--mllib--libmllib_resources.jar:/databricks/jars/----workspace_spark_3_1--mllib--mllib_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--mllib--org.jpmml__pmml-model__1.4.8_shaded-for-mllib.jar:/databricks/jars/----workspace_spark_3_1--mllib-local--mllib-local-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--py4j--py4j-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--repl--repl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--catalyst-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libcatalyst_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--libspark-sql-parser-compiled.jar:/databricks/jars/----workspace_spark_3_1--sql--catalyst--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--core--proto_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--core--spark-sql-databricks-command-parser_java_out.srcjar:/databricks/jars/----workspace_spark_3_1--sql--hive--hive_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--libhive_resources.jar:/databricks/jars/----workspace_spark_3_1--sql--hive--org.apache.commons__commons-pool2__2.6.2_shaded-for-hive.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--sql--hive-thriftserver--hive-thriftserver-protocol-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--streaming--libstreaming_resources.jar:/databricks/jars/----workspace_spark_3_1--streaming--streaming-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--fatJar-assembly-0.18.1-SNAPSHOT.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--bigquery-connector--gcs-connector-hadoop2-2.0.0-shaded.jar_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core-http-netty__1.6.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-core__1.8.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-identity__1.1.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.azure__azure-security-keyvault-keys__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.dataformat__jackson-dataformat-xml__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.datatype__jackson-datatype-jsr310__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.jackson.module__jackson-module-jaxb-annotations__2.11.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml.woodstox__woodstox-core__6.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.fasterxml__aalto-xml__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.github.stephenc.jcip__jcip-annotations__1.0-1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.madgag.spongycastle__core__1.54.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j-persistence-extension__1.0.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.azure__msal4j__1.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.microsoft.sqlserver__mssql-jdbc__9.2.1.jre8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__content-type__2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__lang-tag__1.4.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__nimbus-jose-jwt__8.8_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--com.nimbusds__oauth2-oidc-sdk__7.1.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-buffer__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-http__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-codec__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-handler__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-resolver__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-kqueue-osx-x86_64__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.netty__netty-transport__4.1.51.Final_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor.netty__reactor-netty__0.9.11.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--io.projectreactor__reactor-core__3.3.9.RELEASE_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.activation__jakarta.activation-api__1.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--jakarta.xml.bind__jakarta.xml.bind-api__2.3.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.java.dev.jna__jna-platform__5.6.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__accessors-smart__1.2_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--net.minidev__json-smart__2.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.bouncycastle__bcprov-jdk15on__1.69_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.codehaus.woodstox__stax2-api__4.2.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.jetbrains__annotations__15.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-dom__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-jaxb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdb__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-kdbx__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2-simple__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__KeePassJava2__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.linguafranca.pwdb__database__2.1.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.nanohttpd__nanohttpd__2.3.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.ow2.asm__asm__5.0.4_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--org.simpleframework__simple-xml__2.7.1_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--stax__stax__1.2.0_shaded.jar:/databricks/jars/----workspace_spark_3_1--third_party--mssql--xpp3__xpp3__1.1.3.3_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--avro_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--avro--libavro_resources_shaded-for-avro.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--file-notification-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--file-notification-common--libfile-notification-common_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider--libkafka-0-10-token-provider_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10-token-provider-unshaded_2.12_deploy_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--kafka-0-10_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--vendor--libkafka-0-10-resources_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--org.apache.kafka__kafka-clients__2.6.0_shaded-for-kafka-0-10.jar:/databricks/jars/----workspace_spark_3_1--vendor--redshift--redshift-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--libmetrics-ganglia.jar:/databricks/jars/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--spark-ganglia-lgpl-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--libsql-aws-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-aws-connectors--sql-aws-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--libsql-azure-connectors_resources.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-azure-connectors--sql-azure-connectors-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--vendor--sql-dw--sql-dw-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/acl--api--helpers--helpers-spark_3.1_2.12_deploy.jar:/databricks/jars/acl--auth--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/api--common--workspace-spark_3.1_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.1_2.12_deploy.jar:/databricks/jars/api-base--api-base_java-spark_3.1_2.12_deploy.jar:/databricks/jars/central--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.1_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.1_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--cloudstorage--presigned-url-spark_3.1_2.12_deploy.jar:/databricks/jars/common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.1_2.12_deploy.jar:/databricks/jars/common--crypto-providers--amazon-corretto-crypto-provider--libamazon-corretto-crypto-provider.jar:/databricks/jars/common--hadoop--hadoop-spark_3.1_2.12_deploy.jar:/databricks/jars/common--java-flight-recorder--java-flight-recorder-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/common--jupyter-utils--jupyter_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.1_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--network--network-spark_3.1_2.12_deploy.jar:/databricks/jars/common--node-types--node-types-spark_3.1_2.12_deploy.jar:/databricks/jars/common--path--path-spark_3.1_2.12_deploy.jar:/databricks/jars/common--pricing--pricing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.1_2.12_deploy.jar:/databricks/jars/common--reflection--reflection-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.1_2.12_deploy.jar:/databricks/jars/common--storage-driver-utils--storage_driver_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.1_2.12_deploy.jar:/databricks/jars/common--util--locks-spark_3.1_2.12_deploy.jar:/databricks/jars/credentials-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.1_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.1_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/jsonutil--jsonutil-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.1_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.1_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--ratelimitedlogger--ratelimitedlogger-spark_3.1_2.12_deploy.jar:/databricks/jars/macros--sourcecode--sourcecode-spark_3.1_2.12_deploy.jar:/databricks/jars/managed-catalog--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/maven-trees--amazon-corretto-crypto-provider--software.amazon.cryptools--AmazonCorrettoCryptoProvider-linux-x86_64--software.amazon.cryptools__AmazonCorrettoCryptoProvider-linux-x86_64__1.4.0.jar:/databricks/jars/s3--s3-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--client--client-spark_3.1_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.1_2.12_deploy.jar:/databricks/jars/secret-manager--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--command--command-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.1_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.1--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.1_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.12.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__2.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.4.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__30.0-android_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.14.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.6.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.36.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.6.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.63.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.23.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_javax.annotation_javax.annotation-api_javax.annotation__javax.annotation-api__1.3.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.19_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.2.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/third_party--armeria--service_discovery-resources.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.7.12_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--annotations_shaded.jar:/databricks/jars/third_party--gcs-private--api-common_shaded.jar:/databricks/jars/third_party--gcs-private--auto-value-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--checker-compat-qual_shaded.jar:/databricks/jars/third_party--gcs-private--checker-qual_shaded.jar:/databricks/jars/third_party--gcs-private--commons-codec_shaded.jar:/databricks/jars/third_party--gcs-private--commons-lang3_shaded.jar:/databricks/jars/third_party--gcs-private--commons-logging_shaded.jar:/databricks/jars/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--gcs-private--error_prone_annotations_shaded.jar:/databricks/jars/third_party--gcs-private--failureaccess_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger-system-backend_shaded.jar:/databricks/jars/third_party--gcs-private--flogger_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-connector_shaded.jar:/databricks/jars/third_party--gcs-private--gcs-shaded-spark_3.1_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcsio_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-api-services-storage_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--gcs-private--google-extensions_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs-private--google-http-client_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--gcs-private--google-oauth-client_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-alts_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-api_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-auth_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-context_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-core_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-grpclb_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-protobuf_shaded.jar:/databricks/jars/third_party--gcs-private--grpc-stub_shaded.jar:/databricks/jars/third_party--gcs-private--gson_shaded.jar:/databricks/jars/third_party--gcs-private--guava_shaded.jar:/databricks/jars/third_party--gcs-private--httpclient_shaded.jar:/databricks/jars/third_party--gcs-private--httpcore_shaded.jar:/databricks/jars/third_party--gcs-private--j2objc-annotations_shaded.jar:/databricks/jars/third_party--gcs-private--jackson-core_shaded.jar:/databricks/jars/third_party--gcs-private--javax.annotation-api_shaded.jar:/databricks/jars/third_party--gcs-private--jsr305_shaded.jar:/databricks/jars/third_party--gcs-private--listenablefuture_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-api_shaded.jar:/databricks/jars/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--gcs-private--perfmark-api_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--gcs-private--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java-util_shaded.jar:/databricks/jars/third_party--gcs-private--protobuf-java_shaded.jar:/databricks/jars/third_party--gcs-private--util-hadoop_shaded.jar:/databricks/jars/third_party--gcs-private--util_shaded.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.1_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.1.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.1_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-annotations_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-core_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-databind_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-api_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-impl_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-jackson_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--zeromq--jeromq_shaded.jar:/databricks/jars/third_party--zeromq--jnacl_shaded.jar:/databricks/jars/utils--process_utils-spark_3.1_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.1_2.12_deploy.jar</td></tr><tr><td>spark.databricks.clusterUsageTags.isSingleUserCluster</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterState</td><td>Pending</td></tr><tr><td>spark.databricks.tahoe.logStore.azure.class</td><td>com.databricks.tahoe.store.AzureLogStore</td></tr><tr><td>spark.hadoop.fs.azure.skip.metrics</td><td>true</td></tr><tr><td>spark.hadoop.fs.s3.impl</td><td>shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem</td></tr><tr><td>spark.scheduler.mode</td><td>FAIR</td></tr><tr><td>spark.sql.sources.default</td><td>delta</td></tr><tr><td>spark.hadoop.fs.cpfs-s3n.impl</td><td>com.databricks.sql.acl.fs.CredentialPassthroughFileSystem</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterWorkers</td><td>0</td></tr><tr><td>spark.hadoop.fs.cpfs-adl.impl</td><td>com.databricks.sql.acl.fs.CredentialPassthroughFileSystem</td></tr><tr><td>spark.hadoop.fs.cpfs-abfss.impl</td><td>com.databricks.sql.acl.fs.CredentialPassthroughFileSystem</td></tr><tr><td>spark.databricks.passthrough.oauth.refresher.impl</td><td>com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient</td></tr><tr><td>spark.sql.hive.metastore.sharedPrefixes</td><td>org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks</td></tr><tr><td>spark.databricks.io.directoryCommit.enableLogicalDelete</td><td>false</td></tr><tr><td>spark.task.reaper.killTimeout</td><td>60s</td></tr><tr><td>spark.hadoop.parquet.block.size.row.check.min</td><td>10</td></tr><tr><td>spark.hadoop.hive.server2.use.SSL</td><td>true</td></tr><tr><td>spark.hadoop.fs.mcfs-s3a.impl</td><td>com.databricks.sql.acl.fs.ManagedCatalogFileSystem</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterAvailability</td><td>ON_DEMAND</td></tr><tr><td>spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb</td><td>0</td></tr><tr><td>spark.hadoop.hive.server2.keystore.path</td><td>/databricks/keys/jetty-ssl-driver-keystore.jks</td></tr><tr><td>spark.databricks.credential.redactor</td><td>com.databricks.logging.secrets.CredentialRedactorProxyImpl</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterPinned</td><td>false</td></tr><tr><td>spark.databricks.acl.provider</td><td>com.databricks.sql.acl.ReflectionBackedAclProvider</td></tr><tr><td>spark.extraListeners</td><td>com.databricks.backend.daemon.driver.DBCEventLoggingListener</td></tr><tr><td>spark.sql.parquet.cacheMetadata</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2</td><td>0</td></tr><tr><td>spark.hadoop.fs.adl.impl</td><td>com.databricks.adl.AdlFileSystem</td></tr><tr><td>spark.hadoop.fs.cpfs-abfss.impl.disable.cache</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.enableLocalDiskEncryption</td><td>false</td></tr><tr><td>spark.databricks.tahoe.logStore.class</td><td>com.databricks.tahoe.store.DelegatingLogStore</td></tr><tr><td>libraryDownload.sleepIntervalSeconds</td><td>5</td></tr><tr><td>spark.databricks.cloudProvider</td><td>AWS</td></tr><tr><td>spark.sql.hive.convertMetastoreParquet</td><td>true</td></tr><tr><td>spark.executor.id</td><td>driver</td></tr><tr><td>spark.databricks.service.dbutils.server.backend</td><td>com.databricks.dbconnect.SparkServerDBUtils</td></tr><tr><td>spark.databricks.clusterUsageTags.workerEnvironmentId</td><td>default-worker-env</td></tr><tr><td>spark.databricks.repl.enableClassFileCleanup</td><td>true</td></tr><tr><td>spark.sql.catalogImplementation</td><td>hive</td></tr><tr><td>spark.databricks.managedCatalog.s3a.tokenProviderClassName</td><td>com.databricks.backend.daemon.driver.aws.ManagedCatalogS3TokenProvider</td></tr><tr><td>spark.hadoop.fs.s3a.multipart.size</td><td>10485760</td></tr><tr><td>spark.databricks.clusterUsageTags.cloudProvider</td><td>AWS</td></tr><tr><td>spark.metrics.conf</td><td>/databricks/spark/conf/metrics.properties</td></tr><tr><td>spark.akka.frameSize</td><td>256</td></tr><tr><td>spark.hadoop.fs.s3a.fast.upload</td><td>true</td></tr><tr><td>spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled</td><td>true</td></tr><tr><td>spark.sql.streaming.stopTimeout</td><td>15s</td></tr><tr><td>spark.hadoop.hive.server2.keystore.password</td><td>[REDACTED]</td></tr><tr><td>spark.hadoop.fs.wasbs.impl</td><td>shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td>spark.databricks.overrideDefaultCommitProtocol</td><td>org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterNoDriverDaemon</td><td>false</td></tr><tr><td>libraryDownload.timeoutSeconds</td><td>180</td></tr><tr><td>spark.hadoop.parquet.memory.pool.ratio</td><td>0.5</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterScalingType</td><td>fixed_size</td></tr><tr><td>spark.sql.hive.metastore.jars</td><td>/databricks/hive/*</td></tr><tr><td>spark.databricks.passthrough.adls.gen2.tokenProviderClassName</td><td>com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider</td></tr><tr><td>spark.databricks.tahoe.logStore.gcp.class</td><td>com.databricks.tahoe.store.GCPLogStore</td></tr><tr><td>spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td>spark.sql.sources.commitProtocolClass</td><td>com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol</td></tr><tr><td>spark.hadoop.fs.abfss.impl</td><td>shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterPythonVersion</td><td>3</td></tr><tr><td>spark.databricks.clusterUsageTags.enableDfAcls</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount</td><td>0</td></tr><tr><td>spark.shuffle.service.enabled</td><td>true</td></tr><tr><td>spark.hadoop.fs.file.impl</td><td>com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem</td></tr><tr><td>spark.hadoop.fs.mcfs-s3n.impl</td><td>com.databricks.sql.acl.fs.ManagedCatalogFileSystem</td></tr><tr><td>spark.hadoop.fs.cpfs-s3.impl</td><td>com.databricks.sql.acl.fs.CredentialPassthroughFileSystem</td></tr><tr><td>spark.hadoop.fs.s3a.multipart.threshold</td><td>104857600</td></tr><tr><td>spark.rpc.message.maxSize</td><td>256</td></tr><tr><td>spark.databricks.driverNfs.enabled</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterMetastoreAccessType</td><td>RDS_DIRECT</td></tr><tr><td>spark.databricks.clusterUsageTags.ngrokNpipEnabled</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.instanceProfileUsed</td><td>false</td></tr><tr><td>spark.r.sql.derby.temp.dir</td><td>/tmp/RtmpT4QJ8k</td></tr><tr><td>spark.hadoop.fs.abfs.impl</td><td>shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem</td></tr><tr><td>spark.app.startTime</td><td>1634401753697</td></tr><tr><td>spark.databricks.acl.scim.client</td><td>com.databricks.spark.sql.acl.client.DriverToWebappScimClient</td></tr><tr><td>spark.databricks.clusterUsageTags.driverInstancePrivateIp</td><td>10.172.179.45</td></tr><tr><td>spark.hadoop.fs.adl.impl.disable.cache</td><td>true</td></tr><tr><td>spark.hadoop.parquet.block.size.row.check.max</td><td>10</td></tr><tr><td>spark.hadoop.fs.s3a.connection.maximum</td><td>200</td></tr><tr><td>spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2</td><td>0</td></tr><tr><td>spark.executor.extraJavaOptions</td><td>-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1</td></tr><tr><td>spark.hadoop.fs.s3a.fast.upload.active.blocks</td><td>32</td></tr><tr><td>spark.shuffle.reduceLocality.enabled</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.driverNodeType</td><td>dev-tier-node</td></tr><tr><td>spark.hadoop.spark.sql.sources.outputCommitterClass</td><td>com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter</td></tr><tr><td>spark.hadoop.fs.AbstractFileSystem.gs.impl</td><td>shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</td></tr><tr><td>spark.databricks.clusterUsageTags.instanceBootstrapType</td><td>ssh</td></tr><tr><td>spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled</td><td>false</td></tr><tr><td>spark.databricks.driverNodeTypeId</td><td>dev-tier-node</td></tr><tr><td>spark.sql.parquet.compression.codec</td><td>snappy</td></tr><tr><td>spark.databricks.cloudfetch.hasRegionSupport</td><td>true</td></tr><tr><td>spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories</td><td>false</td></tr><tr><td>spark.hadoop.fs.wasb.impl</td><td>shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td>spark.hadoop.fs.mcfs-abfss.impl.disable.cache</td><td>true</td></tr><tr><td>spark.databricks.workerNodeTypeId</td><td>dev-tier-node</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterEbsVolumeSize</td><td>0</td></tr><tr><td>spark.sql.warehouse.dir</td><td>/user/hive/warehouse</td></tr><tr><td>spark.databricks.passthrough.s3a.tokenProviderClassName</td><td>com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider</td></tr><tr><td>spark.databricks.session.share</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.driverContainerId</td><td>83d618269aee420ca7c77aea0e1025e6</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterResourceClass</td><td>default</td></tr><tr><td>spark.driver.port</td><td>39685</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterSku</td><td>STANDARD_SKU</td></tr><tr><td>spark.hadoop.fs.gs.impl.disable.cache</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterEbsVolumeType</td><td>GENERAL_PURPOSE_SSD</td></tr><tr><td>spark.hadoop.parquet.page.size.check.estimate</td><td>false</td></tr><tr><td>spark.hadoop.spark.driverproxy.customHeadersToProperties</td><td>X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name</td></tr><tr><td>spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class</td><td>com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory</td></tr><tr><td>spark.databricks.delta.preview.enabled</td><td>true</td></tr><tr><td>spark.databricks.cloudfetch.requesterClassName</td><td>com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester</td></tr><tr><td>spark.master</td><td>local[8]</td></tr><tr><td>spark.databricks.delta.logStore.crossCloud.fatal</td><td>true</td></tr><tr><td>spark.files.fetchFailure.unRegisterOutputOnHost</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.sparkVersion</td><td>8.3.x-scala2.12</td></tr><tr><td>spark.databricks.clusterUsageTags.enableSqlAclsOnly</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterEbsVolumeCount</td><td>0</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterNumSshKeys</td><td>0</td></tr><tr><td>spark.hadoop.fs.gs.outputstream.upload.chunk.size</td><td>16777216</td></tr><tr><td>spark.databricks.tahoe.logStore.aws.class</td><td>com.databricks.tahoe.store.S3LockBasedLogStore</td></tr><tr><td>spark.speculation.quantile</td><td>0.9</td></tr><tr><td>spark.databricks.clusterUsageTags.privateLinkEnabled</td><td>false</td></tr><tr><td>spark.shuffle.manager</td><td>SORT</td></tr><tr><td>spark.files.overwrite</td><td>true</td></tr><tr><td>spark.r.numRBackendThreads</td><td>1</td></tr><tr><td>spark.hadoop.fs.wasbs.impl.disable.cache</td><td>true</td></tr><tr><td>spark.hadoop.fs.abfss.impl.disable.cache</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterOwnerUserId</td><td>4661816221656833</td></tr><tr><td>spark.databricks.workspace.multipleResults.enabled</td><td>true</td></tr><tr><td>spark.sql.hive.metastore.version</td><td>0.13.0</td></tr><tr><td>spark.shuffle.service.port</td><td>4048</td></tr><tr><td>spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType</td><td>default</td></tr><tr><td>spark.databricks.acl.client</td><td>com.databricks.spark.sql.acl.client.SparkSqlAclClient</td></tr><tr><td>spark.streaming.driver.writeAheadLog.closeFileAfterWrite</td><td>true</td></tr><tr><td>spark.hadoop.hive.warehouse.subdir.inherit.perms</td><td>false</td></tr><tr><td>spark.hadoop.fs.mcfs-abfss.impl</td><td>com.databricks.sql.acl.fs.ManagedCatalogFileSystem</td></tr><tr><td>spark.hadoop.fs.s3n.impl</td><td>shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem</td></tr><tr><td>spark.databricks.clusterUsageTags.enableElasticDisk</td><td>false</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterId</td><td>1016-162341-65m0wgy</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterNodeType</td><td>dev-tier-node</td></tr><tr><td>spark.databricks.passthrough.adls.tokenProviderClassName</td><td>com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider</td></tr><tr><td>spark.app.name</td><td>Databricks Shell</td></tr><tr><td>spark.driver.allowMultipleContexts</td><td>false</td></tr><tr><td>spark.rdd.compress</td><td>true</td></tr><tr><td>spark.databricks.python.defaultPythonRepl</td><td>ipykernel</td></tr><tr><td>spark.databricks.eventLog.dir</td><td>eventlogs</td></tr><tr><td>spark.databricks.driverNfs.pathSuffix</td><td>.ephemeral_nfs</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterCreator</td><td>Webapp</td></tr><tr><td>spark.speculation</td><td>false</td></tr><tr><td>spark.hadoop.databricks.dbfs.client.version</td><td>v1</td></tr><tr><td>spark.hadoop.hive.server2.session.check.interval</td><td>60000</td></tr><tr><td>spark.sql.hive.convertCTAS</td><td>true</td></tr><tr><td>spark.hadoop.spark.sql.parquet.output.committer.class</td><td>org.apache.spark.sql.parquet.DirectParquetOutputCommitter</td></tr><tr><td>spark.hadoop.fs.gs.impl</td><td>shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</td></tr><tr><td>spark.hadoop.fs.s3a.fast.upload.default</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterGeneration</td><td>0</td></tr><tr><td>spark.hadoop.fs.abfs.impl.disable.cache</td><td>true</td></tr><tr><td>spark.speculation.multiplier</td><td>3</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterOwnerOrgId</td><td>2811288910448988</td></tr><tr><td>spark.storage.blockManagerTimeoutIntervalMs</td><td>300000</td></tr><tr><td>spark.driver.host</td><td>10.172.162.142</td></tr><tr><td>spark.databricks.clusterUsageTags.instanceWorkerEnvId</td><td>default-worker-env</td></tr><tr><td>spark.repl.class.outputDir</td><td>/local_disk0/tmp/repl/spark-5838928010713590200-66bd2a8f-f53a-4a43-ad10-4732052981f2</td></tr><tr><td>spark.databricks.clusterUsageTags.driverContainerPrivateIp</td><td>10.172.162.142</td></tr><tr><td>spark.sparkr.use.daemon</td><td>false</td></tr><tr><td>spark.scheduler.listenerbus.eventqueue.capacity</td><td>20000</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterStateMessage</td><td>Starting Spark</td></tr><tr><td>spark.hadoop.parquet.page.write-checksum.enabled</td><td>true</td></tr><tr><td>spark.hadoop.databricks.s3commit.client.sslTrustAll</td><td>false</td></tr><tr><td>spark.hadoop.fs.s3a.threads.max</td><td>136</td></tr><tr><td>spark.repl.class.uri</td><td>spark://10.172.162.142:39685/classes</td></tr><tr><td>spark.r.backendConnectionTimeout</td><td>604800</td></tr><tr><td>spark.hadoop.hive.server2.idle.session.timeout</td><td>900000</td></tr><tr><td>spark.databricks.redactor</td><td>com.databricks.spark.util.DatabricksSparkLogRedactorProxy</td></tr><tr><td>spark.databricks.clusterUsageTags.autoTerminationMinutes</td><td>120</td></tr><tr><td>spark.hadoop.fs.s3a.impl</td><td>shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem</td></tr><tr><td>spark.hadoop.parquet.page.verify-checksum.enabled</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.dataPlaneRegion</td><td>us-west-2</td></tr><tr><td>spark.logConf</td><td>true</td></tr><tr><td>spark.databricks.clusterUsageTags.enableJobsAutostart</td><td>true</td></tr><tr><td>spark.hadoop.hive.server2.enable.doAs</td><td>false</td></tr><tr><td>eventLog.rolloverIntervalSeconds</td><td>3600</td></tr><tr><td>spark.shuffle.memoryFraction</td><td>0.2</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterAllTags</td><td>[{\"key\":\"Name\",\"value\":\"ce-worker\"}]</td></tr><tr><td>spark.databricks.clusterUsageTags.driverInstanceId</td><td>i-0a4c6c24cf2ad3d92</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterName</td><td>manning_project</td></tr><tr><td>spark.hadoop.fs.cpfs-s3a.impl</td><td>com.databricks.sql.acl.fs.CredentialPassthroughFileSystem</td></tr><tr><td>spark.databricks.clusterUsageTags.region</td><td>us-west-2</td></tr><tr><td>spark.databricks.clusterUsageTags.clusterSpotBidPricePercent</td><td>100</td></tr><tr><td>spark.files.useFetchCache</td><td>false</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We use dbutils.fs to perform the different operations on the Databricks File System\n\n# Create datalake directory under root directory\ndbutils.fs.mkdirs('/datalake')\n\n# Create the 3 directories for the layers of our data lake\ndbutils.fs.mkdirs('/datalake/raw')\ndbutils.fs.mkdirs('/datalake/curated')\ndbutils.fs.mkdirs('/datalake/serving')\n\n# List the contents of /datalake to see our new directories\ndisplay(dbutils.fs.ls('/datalake'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Step #4","showTitle":true,"inputWidgets":{},"nuid":"3a6b1791-2b13-4198-8449-3495809dd227"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/datalake/curated/","curated/",0],["dbfs:/datalake/raw/","raw/",0],["dbfs:/datalake/serving/","serving/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/datalake/curated/</td><td>curated/</td><td>0</td></tr><tr><td>dbfs:/datalake/raw/</td><td>raw/</td><td>0</td></tr><tr><td>dbfs:/datalake/serving/</td><td>serving/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Manning_DataIngestionCleansingProject_Milestone_1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2993371275230771}},"nbformat":4,"nbformat_minor":0}
